{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6e11be10-7228-4a50-a5ad-3d184acc0dea","showTitle":false,"title":""}},"source":["Neural network model (without day 0)\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#datasets without day 0 \n","import pandas as pd\n","Imputed_test_set = pd.read_csv(\"Data/Imputed_test_set.csv\")\n","Imputed_train_set = pd.read_csv(\"Data/Imputed_train_set.csv\")\n","Imputed_train_set_up = pd.read_csv(\"Data/Imputed_train_set_up.csv\")\n","Imputed_validation_set = pd.read_csv(\"Data/Imputed_validation_set.csv\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a8410f8b-a375-47de-aa2c-e9177535447b","showTitle":false,"title":""}},"source":["#Defining x and y values"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fd5aa3e8-39fc-4469-9e8a-c0df807b73da","showTitle":false,"title":""}},"source":["##Train set"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"319632cf-c9fc-437e-b26f-39a8391287e8","showTitle":false,"title":""}},"outputs":[],"source":["#Extracting x and y-values as model input. Sequential sensordata is stored in a 3d matrix of 21 days per 5 features per 365 cows.\n","import numpy as np\n","import pandas as pd\n","#extracting all unique combinations of cow and calving moment \n","Unique_Calvings = Imputed_train_set[['AnimalEartag', 'PaperRecordedCalvingDate']].drop_duplicates() \n","\n","#define sensors\n","feature_names = [\"WalkingTimeMinutesPerDay\", \"EatingTimeMinutesPerDay\", \"LyingTimeMinutesPerDay\", \"StandingTimeMinutesPerDay\", \"RuminationTimeMinutesPerDay\"]\n","\n","#create empty lists per feature to store variables\n","AnimalEartag_list = [] \n","PaperRecordedCalvingDate_list = []\n","CalciumValue_list = []\n","Calciumcluster_list = []\n","BCS_lijst = []\n","Loco_lijst = []\n","SensorWaardes_list = np.zeros((365,21,5)) #define matrix size for sequential features\n","j = 0\n","\n","#itterating through the dataset in order to extract x and y for each cow \n","for index, (AnimalEartag, PaperRecordedCalvingDate) in Unique_Calvings.iterrows():\n","    filter1 = Imputed_train_set['AnimalEartag'] == AnimalEartag\n","    filter2 = PaperRecordedCalvingDate == Imputed_train_set['PaperRecordedCalvingDate']\n","    df_calving = Imputed_train_set[filter1 & filter2]\n","    Loco = df_calving['FirstLocomotionScore'].max()\n","    BCS =  df_calving['FirstBCSScore'].max()\n","    cacluster = df_calving['Calciumcluster'].iloc[-1] \n","    ca = df_calving['Cut_Off'].iloc[-1]\n","    sw = df_calving[feature_names]\n","   \n","    \n","  \n","    #convert to numpy\n","    sw_numpy = np.array(sw)\n","    #add to list \n","    AnimalEartag_list.append(AnimalEartag)\n","    PaperRecordedCalvingDate_list.append(PaperRecordedCalvingDate)\n","    CalciumValue_list.append(ca)\n","    Calciumcluster_list.append(cacluster)\n","    BCS_lijst.append((BCS))\n","    Loco_lijst.append((Loco))\n","    \n","   \n","    SensorWaardes_list[j] = sw_numpy\n","    j = j + 1\n"," \n","#convert to numpy arrays\n","x_train = np.array(SensorWaardes_list)\n","y_train = np.asarray(Calciumcluster_list) #based on cluster method \n","y_train2 = np.asarray(CalciumValue_list) #alternative method of categorisation based on cut-off value\n","x_BCS = np.asarray(BCS_lijst)\n","x_Loco = np.asarray(Loco_lijst)\n","x_static = np.stack((x_BCS, x_Loco), axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"32a8e79b-5c71-4ab6-b71d-7ba218e4b031","showTitle":true,"title":"Onehotencoder"}},"outputs":[],"source":["#Calvingseason and parity are cathegorial variables, however the models functions on numerical variables only, therefore the variables are converted into binairy variables using the sklearn onehotencoder. Each category gets its own colummn and the column of the category that is true gets a one, all other get a zero.     \n","from sklearn.preprocessing import LabelEncoder \n","from sklearn.preprocessing import OneHotEncoder\n","#extract value for each cow\n","static = Imputed_train_set.groupby(['AnimalEartag', 'PaperRecordedCalvingDate']).first()\n","#convert to numpy\n","season = static['CalvingSeason'].to_numpy()\n","Parity = static['Parity'].to_numpy()\n","CalciumDaysInMilk = static['CalciumDaysInMilk'].to_numpy()\n","#define labelencoder\n","labelEnc = LabelEncoder()\n","#fit and apply\n","x_enc = labelEnc.fit_transform(season)\n","x_enc = x_enc.reshape(len(x_enc), 1) \n","onehotEnc = OneHotEncoder(sparse=False)\n","season_encoded = onehotEnc.fit_transform(x_enc)\n","x_enc_P = labelEnc.fit_transform(Parity)\n","x_enc_P = x_enc_P.reshape(len(x_enc_P), 1) \n","parity_encoded = onehotEnc.fit_transform(x_enc_P)\n","#combine season, parity and day of measurement into one numpy array \n","x_static_lean = np.column_stack([season_encoded, parity_encoded, CalciumDaysInMilk])"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"58374257-9b06-4cd5-9a49-1bb70242b577","showTitle":true,"title":"missing value imputation van BCS en Loco "}},"outputs":[],"source":["#BCS and Locomotion score was not measured for every cow, therefore some cows have a null value. The null values are replaced by the value that is most frequent in that colummn. \n","import numpy as np\n","from sklearn.impute import SimpleImputer\n","imp_freq = SimpleImputer(missing_values=0.0, strategy='most_frequent')\n","imp_freq.fit(x_static)\n","x_static = imp_freq.transform(x_static)\n","#all static features are added up into one numpy array \n","x_Static = np.column_stack([x_static, season_encoded, parity_encoded, CalciumDaysInMilk]) "]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9b6b65af-4f19-4744-9fff-95aa73df64df","showTitle":false,"title":""}},"source":["##Train set upsampeld"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"77e946d0-4b43-4660-81ae-1e580d6e1521","showTitle":false,"title":""}},"outputs":[],"source":["#Since this train set is upsampled, it is impossible to filter on animal eartag and paperrecordedcalvingdate (some cows of cathegory 1 are duplicated and therefore no longer unique) therefore a new column was introduced while performing upsampling called 'Samplenumber' In order to extract an x and an y per sample, an itteration is performed based on samplenumber. \n","Unique_Calvings = Imputed_train_set_up['SampleNumber'].drop_duplicates()\n","\n","\n","#creating empty lists, pay attention to the increased matrix size and the addition of a list for samplenumber\n","SampleNumber_list = []\n","CalciumValue_list = []\n","Calciumcluster_list = []\n","SensorWaardes_list = np.zeros((534,21,5)) #534 cows, 21 days, 5 sensors\n","BCS_lijst = []\n","Loco_lijst = []\n","j = 0\n","for s in range(len(Unique_Calvings)):\n","    filter = Imputed_train_set_up['SampleNumber'] == s\n","    df_calving = Imputed_train_set_up[filter]\n","    Loco = df_calving['FirstLocomotionScore'].max()\n","    BCS =  df_calving['FirstBCSScore'].max()\n","    i = 1\n","    cacluster = df_calving['Calciumcluster'].iloc[-i]\n","    ca = df_calving['Cut_Off'].iloc[-i]\n","    i = i + 1 #prevents infinite loop\n","  \n","    sw = df_calving[feature_names]\n","  \n","    #convert to numpy\n","    sw_numpy = np.array(sw)\n","    #add to list\n","    SampleNumber_list.append(s)\n","    CalciumValue_list.append(ca)\n","    Calciumcluster_list.append(cacluster)\n","    BCS_lijst.append((BCS))\n","    Loco_lijst.append((Loco))\n","  \n","    SensorWaardes_list[j] = sw_numpy\n","    j = j + 1\n","\n","#transform to numpy array\n","x_train_up = np.array(SensorWaardes_list)\n","y_train_up = np.asarray(Calciumcluster_list)\n","y_train2_up =np.asarray(CalciumValue_list)\n","x_BCS_up = np.asarray(BCS_lijst) #BCS end dry period \n","x_Loco_up = np.asarray(Loco_lijst) #locomotionscore end dry period\n","x_static_up = np.stack((x_BCS_up, x_Loco_up), axis = 1) #combining BCS and Locomotion in order to be able to perform upsampling efficiently"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"632c89b6-674f-45df-aec9-c8f10f8d9572","showTitle":true,"title":"One hot encoding and imputing BCS  + Locomotionscore for the upsampeld trainset "}},"outputs":[],"source":["#one hot encoding upsampled train set\n","#Calvingseason and parity are cathegorial variables, however the models functions on numerical variables only, therefore the variables are converted into binairy variables using the sklearn onehotencoder. Each category gets its own colummn and the column of the category that is true gets a one, all other get a zero.\n","#based on samplenumber instead of animaleartag due to the upsampling \n","static = Imputed_train_set_up.groupby(['SampleNumber']).first()\n","#convert to numpy arrays\n","season = static['CalvingSeason'].to_numpy()\n","Parity = static['Parity'].to_numpy()\n","CalciumDaysInMilk_up = static['CalciumDaysInMilk'].to_numpy()\n","#define encoder\n","labelEnc = LabelEncoder()\n","#fit and apply\n","x_enc = labelEnc.fit_transform(season)\n","x_enc = x_enc.reshape(len(x_enc), 1) \n","onehotEnc = OneHotEncoder(sparse=False)\n","season_encoded_up = onehotEnc.fit_transform(x_enc)\n","x_enc_P = labelEnc.fit_transform(Parity)\n","x_enc_P = x_enc_P.reshape(len(x_enc_P), 1) \n","parity_encoded_up = onehotEnc.fit_transform(x_enc_P)\n","x_static_up = imp_freq.transform(x_static_up)\n","#combine parity, calving season and day of measurement\n","x_static_up_lean = np.column_stack([season_encoded_up, parity_encoded_up, CalciumDaysInMilk_up])\n","#combine all static features into one array\n","x_Static_up = np.column_stack([x_static_up, season_encoded_up, parity_encoded_up, CalciumDaysInMilk_up]) \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"926669de-312d-4dbd-90a2-cf909962a3eb","showTitle":false,"title":""}},"source":["##Validation set"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8910541b-dde6-458a-a2ad-ce3430cc97c3","showTitle":false,"title":""}},"outputs":[],"source":["#extracting all unique combinations of cow and calving moment  \n","Unique_Calvings_Val = Imputed_validation_set[['AnimalEartag', 'PaperRecordedCalvingDate']].drop_duplicates()\n","\n","\n","AnimalEartagV_list = []\n","PaperRecordedCalvingDateV_list = []\n","CalciumValueV_list = []\n","CalciumclusterV_list = []\n","BCS_lijst = []\n","Loco_lijst = []\n","SensorWaardesV_list = np.zeros((122,21,5)) #122 cows, 21 days, 5 sensors\n","j = 0\n","for index, (AnimalEartag, PaperRecordedCalvingDate) in Unique_Calvings_Val.iterrows():\n","    filter1 = Imputed_validation_set['AnimalEartag'] == AnimalEartag\n","    filter2 = PaperRecordedCalvingDate == Imputed_validation_set['PaperRecordedCalvingDate']\n","    df_calving_val = Imputed_validation_set[filter1 & filter2]\n","    i = 1\n","    Loco = df_calving_val['FirstLocomotionScore'].max()\n","    BCS =  df_calving_val['FirstBCSScore'].max()\n","    caV = df_calving_val['Cut_Off'].iloc[-i]\n","    caclusterV = df_calving_val['Calciumcluster'].iloc[-i]\n","    swV = df_calving_val[feature_names]\n","  \n","    #convert to numpy\n","    swV_numpy = np.array(swV)\n","    #add to list \n","    AnimalEartagV_list.append(AnimalEartag)\n","    PaperRecordedCalvingDateV_list.append(PaperRecordedCalvingDate)\n","    CalciumValueV_list.append(caV)\n","    CalciumclusterV_list.append(caclusterV)\n","    BCS_lijst.append((BCS))\n","    Loco_lijst.append((Loco))\n","    \n","    SensorWaardesV_list[j] = swV_numpy\n","    j = j + 1\n","\n","#convert to numpy \n","x_val = np.array(SensorWaardesV_list)\n","y_val2 = np.asarray(CalciumValueV_list)\n","y_val = np.asarray(CalciumclusterV_list)\n","x_BCS = np.asarray(BCS_lijst)\n","x_Loco = np.asarray(Loco_lijst)\n","#stack BCS en locomotion score \n","x_static_val = np.stack((x_BCS, x_Loco), axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2869697e-38b5-4c98-bf0a-8292a4383faf","showTitle":true,"title":"Onehotencoder en Missing value imputation Validation set "}},"outputs":[],"source":["#one hot encoding validation\n","from sklearn.preprocessing import LabelEncoder \n","from sklearn.preprocessing import OneHotEncoder\n","static = Imputed_validation_set.groupby(['AnimalEartag', 'PaperRecordedCalvingDate']).first()\n","season = static['CalvingSeason'].to_numpy()\n","Parity = static['Parity'].to_numpy()\n","CalciumDaysInMilk = static['CalciumDaysInMilk'].to_numpy()\n","labelEnc = LabelEncoder()\n","x_enc = labelEnc.fit_transform(season)\n","x_enc = x_enc.reshape(len(x_enc), 1) \n","onehotEnc = OneHotEncoder(sparse=False)\n","season_encoded = onehotEnc.fit_transform(x_enc)\n","x_enc_P = labelEnc.fit_transform(Parity)\n","x_enc_P = x_enc_P.reshape(len(x_enc_P), 1) \n","parity_encoded = onehotEnc.fit_transform(x_enc_P)\n","#combine static features \n","x_static_val_lean = np.column_stack([season_encoded, parity_encoded, CalciumDaysInMilk])\n","#impute missing BCS and Locomotion scores \n","x_static_val = imp_freq.transform(x_static_val)\n","# combine BCS and Locomotion scores with the other static features\n","x_Static_val = np.column_stack([x_static_val, season_encoded, parity_encoded,CalciumDaysInMilk]) \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9492db97-4f5c-4c21-aa78-9e99f682c1ee","showTitle":false,"title":""}},"source":["##Test set"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"bea2fded-2bef-403f-9ca4-2f063ae9a1b9","showTitle":false,"title":""}},"outputs":[],"source":["#extracting all unique combinations of cow and calving moment\n","Unique_Calvings_Test = Imputed_test_set[['AnimalEartag', 'PaperRecordedCalvingDate']].drop_duplicates()\n","\n","AnimalEartag_Test_list = []\n","PaperRecordedCalvingDate_Test_list = []\n","CalciumValue_Test_list = []\n","Calciumcluster_Test_list = []\n","BCS_lijst = []\n","Loco_lijst = []\n","SensorWaardes_Test_list = np.zeros((122,21,5))\n","k = 0\n","for index, (AnimalEartag, PaperRecordedCalvingDate) in Unique_Calvings_Test.iterrows():\n","    filter1 = Imputed_test_set['AnimalEartag'] == AnimalEartag\n","    filter2 = PaperRecordedCalvingDate == Imputed_test_set['PaperRecordedCalvingDate']\n","    df_calving_test = Imputed_test_set[filter1 & filter2]\n","    Loco = df_calving_test['FirstLocomotionScore'].max()\n","    BCS =  df_calving_test['FirstBCSScore'].max()\n","    i = 1\n","    catest = df_calving_test['Cut_Off'].iloc[-i]\n","    caclustertest = df_calving_test['Calciumcluster'].iloc[-i]\n","    a = df_calving_test['CalvingSeason'].dropna()\n","  \n","    swtest = df_calving_test[feature_names]\n","\n","    #convert to numpy\n","    swtest_numpy = np.array(swtest)\n","\n","    #add to list \n","    AnimalEartag_Test_list.append(AnimalEartag)\n","    PaperRecordedCalvingDate_Test_list.append(PaperRecordedCalvingDate)\n","    CalciumValue_Test_list.append(catest)\n","    Calciumcluster_Test_list.append(caclustertest)\n","    BCS_lijst.append((BCS))\n","    Loco_lijst.append((Loco))\n","    SensorWaardes_Test_list[k] = swtest_numpy\n","    k = k + 1\n","\n","\n","x_test = np.asarray(SensorWaardes_Test_list)\n","y_test2 = np.asarray(CalciumValue_Test_list)\n","y_test = np.asarray(Calciumcluster_Test_list)\n","x_BCS = np.asarray(BCS_lijst)\n","x_Loco = np.asarray(Loco_lijst)\n","x_static_test = np.stack((x_BCS, x_Loco), axis = 1)\n","#impute missing BCS and Locomotion scores\n","x_static_test = imp_freq.transform(x_static_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c46f40a1-90dd-4f5c-b6d7-8c4e5bccbc23","showTitle":true,"title":"Onehotencoder en Missing value imputation Test set "}},"outputs":[],"source":["#one hot encoding test set\n","static = Imputed_test_set.groupby(['AnimalEartag', 'PaperRecordedCalvingDate']).first()\n","season = static['CalvingSeason'].to_numpy()\n","Parity = static['Parity'].to_numpy()\n","CalciumDaysInMilk = static['CalciumDaysInMilk'].to_numpy()\n","labelEnc = LabelEncoder()\n","x_enc = labelEnc.fit_transform(season)\n","x_enc = x_enc.reshape(len(x_enc), 1) \n","onehotEnc = OneHotEncoder(sparse=False)\n","season_encoded = onehotEnc.fit_transform(x_enc)\n","x_enc_P = labelEnc.fit_transform(Parity)\n","x_enc_P = x_enc_P.reshape(len(x_enc_P), 1) \n","parity_encoded = onehotEnc.fit_transform(x_enc_P)\n","#combine static features\n","x_static_test_lean = np.column_stack([season_encoded, parity_encoded, CalciumDaysInMilk])\n","# combine static features with BCS and Locomotion scores\n","x_Static_test = np.column_stack([x_static_test, season_encoded, parity_encoded, CalciumDaysInMilk]) \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9fea7b44-657b-42ce-9df3-fe6e4f4fabaa","showTitle":false,"title":""}},"source":["#Bootstrapping"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"262723c7-9d6b-4c15-bbf9-8097cee4762c","showTitle":false,"title":""}},"outputs":[],"source":["#XgBoost and Deep learning models have a form of randomness in their initialisation. The exact same model structure with the same training data may therefore give different results each time it is run. In order to be able to compare different models, model performance is not objective enough since it can differ with the same model and could be a high value simply because you were lucky. To compansate for this behaviour a second metric is proposed to compare models; the variance of the model. When a model has high variance, the chances are higher that a high value is obtained by luck and it could be that the second time the model is run, the exact same model results in dramaticly low performance metrics. To be able to measure varriance bootstraps are built, bootstraps are samples from the validation set with the same size as the validation set but acquired with sampling with replacement. Therefore the same model can be tested on multiple validation sets and the results can be compared and the SD calculated. \n","\n","#define function for creating bootstraps\n","def create_bootstrap(x_sensor,x_static,y1, y2):\n","    #initialise empty list for bootstraps\n","    bootstrap_x_sensor = []\n","    bootstrap_x_static = []\n","    bootstrap_y1 = []\n","    bootstrap_y2 = []\n","    \n","    #required length of bootstrap \n","    len_val = x_val.shape[0]\n","    \n","    #get random observation \n","    for i in range(len_val):\n","        # get random index\n","        random_idx = np.random.choice(range(len_val), 1)\n","\t\t# get random observation\n","        random_x_sensor = x_sensor[random_idx]\n","        random_x_static = x_static[random_idx]\n","        random_y1 = y1[random_idx]\n","        random_y2 = y2[random_idx]\n","        \n","\t\t# add random observation to bootstrap\n","        bootstrap_x_sensor.append(random_x_sensor)\n","        bootstrap_x_static.append(random_x_static)\n","        bootstrap_y1.append(random_y1)\n","        bootstrap_y2.append(random_y2)\n","        \n","\t# convert to numpy\n","    bootstrap_x_sensor = np.asarray(bootstrap_x_sensor) \n","    bootstrap_x_static = np.asarray(bootstrap_x_static)\n","    bootstrap_y1 = np.asarray(bootstrap_y1)\n","    bootstrap_y2 = np.asarray(bootstrap_y2)\n","\n","\t# return\t\n","    return(bootstrap_x_sensor, bootstrap_x_static, bootstrap_y1, bootstrap_y2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a6171361-77cf-40e2-a058-922455d6703e","showTitle":false,"title":""}},"outputs":[],"source":["# define function to create bootstraps\n","def create_bootstraps(x_sensor,x_static,y1, y2, number_bootstraps):\n","\t\n","# initialize bootstrap containers\n","    bootstrap_container_x_sensor = []\n","    bootstrap_container_x_static = []\n","    bootstrap_container_y1 = []\n","    bootstrap_container_y2 = []\n","\t\t\n","\t# create n bootstrap\n","    for i in range(number_bootstraps):\n","\t\t# get bootstrap\n","        bootstrap_x_sensor, bootstrap_x_static, bootstrap_y1, bootstrap_y2 = create_bootstrap(x_sensor,x_static,y1, y2)\n","\t\t# add to container\n","        bootstrap_container_x_sensor.append(bootstrap_x_sensor)\n","        bootstrap_container_x_static.append(bootstrap_x_static)\n","        bootstrap_container_y1.append(bootstrap_y1)\n","        bootstrap_container_y2.append(bootstrap_y2)\n","\n","\t# return\n","    return(bootstrap_container_x_sensor, bootstrap_container_x_static, bootstrap_container_y1, bootstrap_container_y2)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"caf6b9e9-4dfa-4ca4-92a2-3508dcb8a930","showTitle":false,"title":""}},"outputs":[],"source":["#define function to evaluate model \n","def evaluate_model(model, bootstrap_container_x_sensor, bootstrap_container_x_static, bootstrap_container_y1, bootstrap_container_y2):\n","\n","\t# initialize evaluation container\n","    performance_container = []\n","\n","\t# loop through bootstraps\n","    for i in range(len(bootstrap_container_x_sensor)):\n","\n","\t\t# get X\n","        bootstrap_x_sensor = bootstrap_container_x_sensor[i]\n","        bootstrap_x_static = bootstrap_container_x_static[i]\n","\t\t# get y\n","        bootstrap_y1 = bootstrap_container_y1[i]\n","        bootstrap_y2 = bootstrap_container_y2[i]\n","        #reshape x from 3d to 2d for machine learning and from 4d to 3d for deep learning \n","        bootstrap_x_sensor = bootstrap_x_sensor.reshape((bootstrap_x_sensor.shape[0], (bootstrap_x_sensor.shape[1]*bootstrap_x_sensor.shape[2]), bootstrap_x_sensor.shape[3])) \n","        bootstrap_x_static = bootstrap_x_static.reshape(bootstrap_x_static.shape[0], (bootstrap_x_static.shape[1]*bootstrap_x_static.shape[2]))\n","        #for XgBoost convert to d-matrix, comment out when not using XgBoost!\n","#         bootstrap_x_sensor =  xgb.DMatrix(bootstrap_x_sensor)\n","\t\t# get predictions #depending on model to evaluate, for some models static features need to be added, for log reg model do not use predict but predict_proba  \n","        preds = model.predict([bootstrap_x_sensor, bootstrap_x_static], verbose = 0)\n","        # preds = model.predict(bootstrap_x_sensor, verbose = 0)\n","\t\t# get metric # first choose witch y set to test 1 = clustered, 2 = cut-off\n","        auc = roc_auc_score(bootstrap_y2, preds)\n","        \n","\t\t# add to container\n","        performance_container.append(auc)\n","      \n","\t# return\n","    return(performance_container)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6233437e-457f-414c-b614-accabaddb3ab","showTitle":false,"title":""}},"source":["#Neural network model"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"11d8f894-57da-43ea-9407-52e55d8796f4","showTitle":false,"title":""}},"outputs":[],"source":["#packages needed for model building, while using databricks pay attention to the fact that you need a machine learning cluster instead of the deafault cluster\n","import tensorflow as tf \n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.layers import Dense, Dropout, concatenate, BatchNormalization\n","from matplotlib import pyplot as plt\n","from tensorflow import keras\n","from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f513b052-9baf-4487-98b7-e5128e6c74aa","showTitle":false,"title":""}},"source":["## Sequential model"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c809b4d4-f421-4ad9-837b-f2a5ea1ccd4c","showTitle":false,"title":""}},"outputs":[],"source":["class_weights = {0:1, 1:1} #used to deal with classinbalance "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c0ff1a2e-4c1d-4e41-af8e-393285e5a55e","showTitle":false,"title":""}},"outputs":[],"source":["#define earlystopping callback\n","my_callbacks = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss',\n","    patience=3,\n","    restore_best_weights = True,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f08ee987-c1a5-4f0d-b12c-435748a05841","showTitle":false,"title":""}},"outputs":[],"source":["#basic sequential model \n","#define model\n","model = Sequential()\n","model.add(LSTM(90, activation = 'relu', input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=False)) #left relu activation out since tanh en sigmoid already cause non linearity\n","model.add(BatchNormalization())\n","model.add(Dropout(0.4)) \n","model.add(Dense(1, activation = \"sigmoid\")) #Sigmoid activation causes binary result \n","#compile model\n","model.compile(optimizer = 'Adam', loss='binary_crossentropy', metrics = ['AUC', 'Precision', 'Recall']) \n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f03fccd6-7bc0-4af8-994d-24aedd346968","showTitle":false,"title":""}},"outputs":[],"source":["# fit model, added earlystopping and class weights\n","history = model.fit(x_train_up, y_train2_up, epochs=100, batch_size = 12, validation_data=(x_val, y_val2), class_weight = {0:1, 1:2}, callbacks = [my_callbacks]) \n","\n","#plot loss and val_loss for every epoch \n","plt.plot(history.history['loss'], label='Training loss')\n","plt.plot(history.history['val_loss'], label='Validation loss')\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d5f048a9-712b-45fd-b8ea-be129b37a001","showTitle":false,"title":""}},"outputs":[],"source":["#Bootstrapping \n","#create 50 bootstraps for deep learning \n","Bootstrap_x_sensor, Bootstrap_x_static, Bootstrap_y1, Bootstrap_y2  = create_bootstraps(x_val, x_static_val_lean, y_val, y_val2, 50) \n","Bootstrap_performance = evaluate_model(model, Bootstrap_x_sensor, Bootstrap_x_static, Bootstrap_y1, Bootstrap_y2)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0730e457-941c-403c-b311-568f6232c3c7","showTitle":false,"title":""}},"outputs":[],"source":["#results bootstrapping, mean auc and std \n","Mean_performance = np.mean(Bootstrap_performance)\n","SD_performance = np.std(Bootstrap_performance)\n","# df = pd.DataFrame(Bootstrap_performance, columns = ['AUC'])\n","# display(df[df[\"AUC\"] < 0.5])\n","Mean_performance, SD_performance, my_callbacks.stopped_epoch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import scipy.stats as st\n","# create 95% confidence interval\n","st.t.interval(confidence=0.95, df = len(Bootstrap_performance), loc = Mean_performance, scale = st.sem(Bootstrap_performance))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"55da6727-53c4-4f8e-9d3e-cac99ce91304","showTitle":false,"title":""}},"outputs":[],"source":["#predict values test set \n","pred = model.predict(x_test)\n","pred_clases = np.where(np.squeeze(pred) < 0.56, 0, 1) "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"47334316-627e-4405-a2cb-d49c4d457e09","showTitle":false,"title":""}},"outputs":[],"source":["#visualize model performance\n","CM = confusion_matrix(y_test2, pred_clases)\n","disp = ConfusionMatrixDisplay(confusion_matrix=CM)\n","disp.plot()\n","print(classification_report(y_test2, pred_clases))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"42cd1ed6-e587-4673-9ec2-0fe5277cbe38","showTitle":false,"title":""}},"outputs":[],"source":["#evaluation metrics\n","sensitiviteit = CM[1,1]/(CM[1,0]+CM[1,1])\n","specificiteit = CM[0,0]/(CM[0,0]+CM[0,1])\n","ppv = CM[1,1]/(CM[1,1]+CM[0,1])\n","auc = roc_auc_score(y_test2, pred)\n","average_precision = average_precision_score(y_test2, pred)\n","print('sensitiviteit =', sensitiviteit, 'specificiteit =', specificiteit, 'ppv =', ppv, 'AUC =', auc, 'average_precision = ', average_precision)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"05067258-cbb5-4036-a848-59206ebc8f2e","showTitle":false,"title":""}},"outputs":[],"source":["#precission recall plot\n","from sklearn.metrics import precision_recall_curve, average_precision_score, auc, plot_precision_recall_curve\n","from matplotlib import pyplot as plt\n","#calculate precision and recall\n","precision, recall, thresholds = precision_recall_curve(y_test, pred_clases)\n","\n","#create precision recall curve\n","fig, ax = plt.subplots()\n","ax.plot(recall, precision, color='purple')\n","\n","#add axis labels to plot\n","ax.set_title('Precision-Recall Curve')\n","ax.set_ylabel('Precision')\n","ax.set_xlabel('Recall')\n","\n","#display plot\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"7322eeba-9731-4c1d-ba0d-04d466659ef8","showTitle":false,"title":""}},"source":["### Hyperparameter tuning sequential model"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"697a7c31-7de3-4dc8-9e55-8ab1c7cb6a75","showTitle":false,"title":""}},"outputs":[],"source":["#define class weight options\n","cw1 = {0:1, 1:1}  \n","cw2 = {0:1, 1:2}\n","cw3 = {0:1, 1:3}\n","cw4 = {0:1, 1:4}"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"962970d7-389e-4eda-9ebe-59b74cae58dc","showTitle":false,"title":""}},"outputs":[],"source":["#def parameters to tune\n","p_dropout = [0, 0.1, 0.2, 0.3, 0.4]\n","p_units = [10, 20, 30, 40, 50, 60, 70, 80,90,100]\n","p_BN = [1,0]\n","p_BS = [12, 22, 32, 42]\n","p_cw = [cw1, cw2, cw3, cw4]"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4b9cdc93-9278-4c13-abea-c7026e61138b","showTitle":true,"title":"non upsampled, y-set 1 "}},"outputs":[],"source":["# A functional model was used for hyperparameter tuning, works at the same way as a sequential model but is compatinble with multi-input and therefore needed when adding static features.  \n","import random\n","#define inputs, in this case single input model\n","sensor_input = keras.Input(shape=(21,5))\n","#create empty list\n","Results = []\n","#run multiple models but with a different random configuration of parameters each time\n","for i in range(200):\n","    units = random.choice(p_units)\n","    BN = random.choice(p_BN)\n","    dropout = random.choice(p_dropout)\n","    BS = random.choice(p_BS)\n","    Class_weights = random.choice(p_cw)\n","#define LSTM:\n","\n","    LSTM_output = LSTM(units)(sensor_input) #relu toevoegen? \n","\n","    if BN == 1:\n","        BN_output = BatchNormalization()(LSTM_output)\n","    else: \n","        BN_output = LSTM_output\n","\n","#dropout layer\n","    Dropout_output = Dropout(dropout)(BN_output)\n","\n","#define final layer\n","    prediction =Dense(1, activation = 'sigmoid')(Dropout_output)\n","\n","#define model\n","    Model = keras.Model(\n","    inputs = sensor_input,\n","    outputs =prediction)\n","#compile model\n","    Model.compile(optimizer = 'Adam', loss='binary_crossentropy',  metrics = ['AUC', 'Precision', 'Recall']) \n","#fit model, verbose set to zero to speed up the process, verbose = 1 shows learning process with intermediate AUC scores of train set \n","    Model.fit(x_train, y_train, batch_size = BS, epochs=100, class_weight = Class_weights, validation_data=(x_val, y_val), callbacks = [my_callbacks],  use_multiprocessing = True,  verbose = 0 )\n","#predict values validation set \n","    pred = Model.predict(x_val, verbose = 0)\n","    pred_clases = np.where(np.squeeze(pred) < 0.5, 0, 1) \n","#add predictions to list and the associated hyperparameters    \n","    Results.append({\n","        'f1_score': f1_score(y_val, pred_clases),\n","        'AUC': roc_auc_score(y_val, pred), \n","        'Average Precision': average_precision_score(y_val, pred), \n","        'p_units': units,\n","        'p_dropout': dropout,\n","        'p_BN' : BN,\n","        'p_BS' : BS,\n","        'p_cw' : Class_weights\n","        })\n","#convert to DF\n","Results = pd.DataFrame(Results)\n","#sort modelresults\n","Results = Results.sort_values(by='AUC', ascending=False)\n","display(Results)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"dc624149-e6c8-4341-904e-036c537b0ea8","showTitle":true,"title":"non upsampled, y-set 2"}},"outputs":[],"source":["# A functional model was used for hyperparameter tuning, works at the same way as a sequential model but is compatinble with multi-input and therefore needed when adding static features.  \n","import random\n","#define inputs, in this case single input model\n","sensor_input = keras.Input(shape=(21,5))\n","#create empty list\n","Results = []\n","#run multiple models but with a different random configuration of parameters each time\n","for i in range(200):\n","    units = random.choice(p_units)\n","    BN = random.choice(p_BN)\n","    dropout = random.choice(p_dropout)\n","    BS = random.choice(p_BS)\n","    Class_weights = random.choice(p_cw)\n","#define LSTM:\n","\n","    LSTM_output = LSTM(units)(sensor_input) #relu toevoegen? \n","\n","    if BN == 1:\n","        BN_output = BatchNormalization()(LSTM_output)\n","    else: \n","        BN_output = LSTM_output\n","\n","#dropout layer\n","    Dropout_output = Dropout(dropout)(BN_output)\n","\n","#define final layer\n","    prediction =Dense(1, activation = 'sigmoid')(Dropout_output)\n","\n","#define model\n","    Model = keras.Model(\n","    inputs = sensor_input,\n","    outputs =prediction)\n","#compile model\n","    Model.compile(optimizer = 'Adam', loss='binary_crossentropy',  metrics = ['AUC', 'Precision', 'Recall']) \n","#fit model, verbose set to zero to speed up the process, verbose = 1 shows learning process with intermediate AUC scores of train set \n","    Model.fit(x_train, y_train2, batch_size = BS, epochs=100, class_weight = Class_weights, validation_data=(x_val, y_val2), callbacks = [my_callbacks],  use_multiprocessing = True,  verbose = 0 )\n","#predict values validation set \n","    pred = Model.predict(x_val, verbose = 0)\n","    pred_clases = np.where(np.squeeze(pred) < 0.5, 0, 1) \n","#add predictions to list and the associated hyperparameters    \n","    Results.append({\n","        'f1_score': f1_score(y_val2, pred_clases),\n","        'AUC': roc_auc_score(y_val2, pred), \n","        'Average Precision': average_precision_score(y_val2, pred), \n","        'p_units': units,\n","        'p_dropout': dropout,\n","        'p_BN' : BN,\n","        'p_BS' : BS,\n","        'p_cw' : Class_weights\n","        })\n","#convert to DF\n","Results = pd.DataFrame(Results)\n","#sort modelresults\n","Results = Results.sort_values(by='AUC', ascending=False)\n","display(Results)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b968844f-b64f-443b-80ee-8ca1d4dd175d","showTitle":true,"title":"Upsampled, y-set 1 "}},"outputs":[],"source":["# A functional model was used for hyperparameter tuning, works at the same way as a sequential model but is compatinble with multi-input and therefore needed when adding static features.  \n","import random\n","#define inputs, in this case single input model\n","sensor_input = keras.Input(shape=(21,5))\n","#create empty list\n","Results = []\n","#run multiple models but with a different random configuration of parameters each time\n","for i in range(200):\n","    units = random.choice(p_units)\n","    BN = random.choice(p_BN)\n","    dropout = random.choice(p_dropout)\n","    BS = random.choice(p_BS)\n","    Class_weights = random.choice(p_cw)\n","#define LSTM:\n","\n","    LSTM_output = LSTM(units)(sensor_input) #relu toevoegen? \n","\n","    if BN == 1:\n","        BN_output = BatchNormalization()(LSTM_output)\n","    else: \n","        BN_output = LSTM_output\n","\n","#dropout layer\n","    Dropout_output = Dropout(dropout)(BN_output)\n","\n","#define final layer\n","    prediction =Dense(1, activation = 'sigmoid')(Dropout_output)\n","\n","#define model\n","    Model = keras.Model(\n","    inputs = sensor_input,\n","    outputs =prediction)\n","#compile model\n","    Model.compile(optimizer = 'Adam', loss='binary_crossentropy',  metrics = ['AUC', 'Precision', 'Recall']) \n","#fit model, verbose set to zero to speed up the process, verbose = 1 shows learning process with intermediate AUC scores of train set \n","    Model.fit(x_train_up, y_train_up, batch_size = BS, epochs=100, class_weight = Class_weights, validation_data=(x_val, y_val), callbacks = [my_callbacks],  use_multiprocessing = True,  verbose = 0 )\n","#predict values validation set \n","    pred = Model.predict(x_val, verbose = 0)\n","    pred_clases = np.where(np.squeeze(pred) < 0.5, 0, 1) \n","#add predictions to list and the associated hyperparameters    \n","    Results.append({\n","        'f1_score': f1_score(y_val, pred_clases),\n","        'AUC': roc_auc_score(y_val, pred), \n","        'Average Precision': average_precision_score(y_val, pred), \n","        'p_units': units,\n","        'p_dropout': dropout,\n","        'p_BN' : BN,\n","        'p_BS' : BS,\n","        'p_cw' : Class_weights\n","        })\n","#convert to DF\n","Results = pd.DataFrame(Results)\n","#sort modelresults\n","Results = Results.sort_values(by='AUC', ascending=False)\n","display(Results)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"24fde947-1a0e-46da-9561-8504bec9aacd","showTitle":true,"title":"upsampled, y-set 2"}},"outputs":[],"source":["# A functional model was used for hyperparameter tuning, works at the same way as a sequential model but is compatinble with multi-input and therefore needed when adding static features.  \n","import random\n","#define inputs, in this case single input model\n","sensor_input = keras.Input(shape=(21,5))\n","#create empty list\n","Results = []\n","#run multiple models but with a different random configuration of parameters each time\n","for i in range(200):\n","    units = random.choice(p_units)\n","    BN = random.choice(p_BN)\n","    dropout = random.choice(p_dropout)\n","    BS = random.choice(p_BS)\n","    Class_weights = random.choice(p_cw)\n","#define LSTM:\n","\n","    LSTM_output = LSTM(units)(sensor_input) #relu toevoegen? \n","\n","    if BN == 1:\n","        BN_output = BatchNormalization()(LSTM_output)\n","    else: \n","        BN_output = LSTM_output\n","\n","#dropout layer\n","    Dropout_output = Dropout(dropout)(BN_output)\n","\n","#define final layer\n","    prediction =Dense(1, activation = 'sigmoid')(Dropout_output)\n","\n","#define model\n","    Model = keras.Model(\n","    inputs = sensor_input,\n","    outputs =prediction)\n","#compile model\n","    Model.compile(optimizer = 'Adam', loss='binary_crossentropy',  metrics = ['AUC', 'Precision', 'Recall']) \n","#fit model, verbose set to zero to speed up the process, verbose = 1 shows learning process with intermediate AUC scores of train set \n","    Model.fit(x_train_up, y_train2_up, batch_size = BS, epochs=100, class_weight = Class_weights, validation_data=(x_val, y_val2), callbacks = [my_callbacks],  use_multiprocessing = True,  verbose = 0 )\n","#predict values validation set \n","    pred = Model.predict(x_val, verbose = 0)\n","    pred_clases = np.where(np.squeeze(pred) < 0.5, 0, 1) \n","#add predictions to list and the associated hyperparameters    \n","    Results.append({\n","        'f1_score': f1_score(y_val2, pred_clases),\n","        'AUC': roc_auc_score(y_val2, pred), \n","        'Average Precision': average_precision_score(y_val2, pred), \n","        'p_units': units,\n","        'p_dropout': dropout,\n","        'p_BN' : BN,\n","        'p_BS' : BS,\n","        'p_cw' : Class_weights\n","        })\n","#convert to DF\n","Results = pd.DataFrame(Results)\n","#sort modelresults\n","Results = Results.sort_values(by='AUC', ascending=False)\n","display(Results)"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"64c854f5-5e41-4f70-a38c-fbffb50ee056","showTitle":false,"title":""}},"source":["##Functional model"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6f828f83-a7aa-49d7-9c97-1938f1e49cba","showTitle":false,"title":""}},"outputs":[],"source":["#Multi-input functional model with both sequential as static input\n","from keras.layers import Dense, LSTM, Dropout, BatchNormalization\n","#define inputs\n","sensor_input = keras.Input(shape=(21,5))\n","static_input = keras.Input(shape=(8))\n","\n","#define LSTM \n","LSTM_output = LSTM(100)(sensor_input)\n","#add batch normalisation \n","BN_output = BatchNormalization()(LSTM_output)\n","#combine with static features\n","all_features = concatenate([BN_output, static_input])\n","\n","#define MLP-layer  \n","MLP_output = Dense(60, activation = 'relu')(all_features)\n","\n","#add dropout \n","Dropout_output = Dropout(rate = 0.4)(MLP_output)\n","\n","#define final layer \n","prediction = Dense(1, activation = 'sigmoid')(Dropout_output)\n","\n","#define model\n","Model = keras.Model(\n","    inputs = [sensor_input, static_input],\n","    outputs = prediction)\n","#compile model\n","Model.compile(optimizer = 'Adam', loss='binary_crossentropy',  metrics = ['AUC', 'Precision', 'Recall'])\n","Model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fc68769b-c429-4b0e-a51a-a421d79ca1f6","showTitle":false,"title":""}},"outputs":[],"source":["#fit model, add class_weights and earlystopping as callback (my_callbacks has been previously defined)\n","history = Model.fit([x_train_up, x_static_up_lean], y_train2_up, batch_size = 12, epochs=100, class_weight = {0:1, 1:2}, validation_data=([x_val, x_static_val_lean], y_val2), callbacks = [my_callbacks], use_multiprocessing = True )\n","#plot loss and val_loss curve to keep track of model functioning\n","plt.plot(history.history['loss'], label='Training loss')\n","plt.plot(history.history['val_loss'], label='Validation loss')\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b11242a5-ebbe-4216-acfc-8480f560ec3f","showTitle":false,"title":""}},"outputs":[],"source":["#Bootstrapping \n","#create 50 bootstraps for deep learning\n","Bootstrap_x_sensor, Bootstrap_x_static, Bootstrap_y1, Bootstrap_y2  = create_bootstraps(x_val, x_static_val_lean, y_val, y_val2, 50) \n","Bootstrap_performance = evaluate_model(Model, Bootstrap_x_sensor, Bootstrap_x_static, Bootstrap_y1, Bootstrap_y2)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"adaaadba-4a50-4ff3-a389-f1c84c5414bf","showTitle":false,"title":""}},"outputs":[],"source":["#results bootstrapping, mean auc and std \n","Mean_performance = np.mean(Bootstrap_performance)\n","SD_performance = np.std(Bootstrap_performance)\n","df = pd.DataFrame(Bootstrap_performance, columns = ['AUC'])\n","# display(df)\n","Mean_performance, SD_performance, my_callbacks.stopped_epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"14ccc2bc-aff2-4d50-a8e4-76ab0bb38cf5","showTitle":false,"title":""}},"outputs":[],"source":["#testing final model on the test set \n","pred = Model.predict([x_test, x_static_test_lean])\n","pred_clases = np.where(np.squeeze(pred) < 0.5, 0, 1) \n","CM = confusion_matrix(y_test2, pred_clases)\n","disp = ConfusionMatrixDisplay(confusion_matrix=CM)\n","disp.plot()\n","print(classification_report(y_test2, pred_clases))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b1549f10-7f58-4c6a-8424-b819bd9849b1","showTitle":false,"title":""}},"outputs":[],"source":["#evaluation metrics \n","sensitiviteit = CM[1,1]/(CM[1,0]+CM[1,1])\n","specificiteit = CM[0,0]/(CM[0,0]+CM[0,1])\n","ppv = CM[1,1]/(CM[1,1]+CM[0,1])\n","auc = roc_auc_score(y_test2, pred) \n","\n","average_precision = average_precision_score(y_test2, pred)\n","print('sensitiviteit =', sensitiviteit, 'specificiteit =', specificiteit, 'ppv =', ppv, 'AUC =', auc, 'average_precision = ', average_precision)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#cummalative accuracy, dependent of threshold chosen and\n","from sklearn.metrics import accuracy_score \n","np.shape(y_test2)\n","# np.shape(pred)\n","y_test2 = y_test2.reshape(y_test2.shape[0], 1)\n","pred_clases = pred_clases.reshape(pred_clases.shape[0], 1) \n","Pred = np.concatenate((pred, pred_clases,y_test2), axis =1)\n","PredPd = pd.DataFrame(Pred, columns= ['result', 'Prediction', 'True'])\n","Sorted = PredPd.sort_values(by=['result'], ascending=False)\n","#pick only the predictions that the model is most sure about\n","subselection = Sorted.iloc[0:30]\n","accuracy = subselection['True'].sum()/subselection['Prediction'].sum()*100\n","total = accuracy_score(y_test2, pred_clases)\n","sub = accuracy_score(subselection['True'], subselection['Prediction'])\n","print('accuracy =', accuracy,'%', 'total =', total, 'alternative =', sub)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"e0a289eb-aeb5-4b46-8b54-f75d03a8af35","showTitle":false,"title":""}},"outputs":[],"source":["from sklearn.metrics import roc_curve\n","fpr, tpr, _ = roc_curve(y_test2,  pred)\n","\n","#create ROC curve\n","plt.plot(fpr,tpr)\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0f44a4ce-9f39-4abd-9fda-580a66dd8e4f","showTitle":false,"title":""}},"outputs":[],"source":["#make a precision recall plot \n","from sklearn.metrics import precision_recall_curve, average_precision_score, auc, plot_precision_recall_curve\n","from matplotlib import pyplot as plt\n","#calculate precision and recall\n","precision, recall, thresholds = precision_recall_curve(y_test2, pred )\n","\n","#create precision recall curve\n","fig, ax = plt.subplots()\n","ax.plot(recall, precision, color='purple')\n","\n","#add axis labels to plot\n","ax.set_title('Precision-Recall Curve')\n","ax.set_ylabel('Precision')\n","ax.set_xlabel('Recall')\n","\n","#display plot\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b2342d0b-2a5e-4783-9788-8c3ef14644ef","showTitle":false,"title":""}},"source":["###hyperparameter tuning functional model"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ab4093d0-15ab-44eb-bbfb-23b7e621ff50","showTitle":false,"title":""}},"outputs":[],"source":["#define possible class-weights\n","cw1 = {0:1, 1:1} \n","cw2 = {0:1, 1:2}\n","cw3 = {0:1, 1:3}\n","cw4 = {0:1, 1:3}"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d14dc39d-518b-423d-aaf3-860a1505c614","showTitle":false,"title":""}},"outputs":[],"source":["#define possible hyperparameters\n","p_dropout = [0, 0.1, 0.2, 0.3, 0.4]\n","p_units = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n","p_units2 = [10, 20, 30, 40, 50, 60, 70, 80]\n","p_BN = [0,1]\n","p_BS = [12, 22, 32, 42]\n","p_cw = [cw1, cw2, cw3, cw4]"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"50dcd652-591c-4d79-ac77-410e8eee90a3","showTitle":true,"title":"func, static 8, non up, y-set 1"}},"outputs":[],"source":["#hyperparameter tuning using random search for multi-input functional model\n","from keras.layers import Dense, LSTM, Dropout, BatchNormalization\n","import random\n","#define inputs\n","sensor_input = keras.Input(shape=(21,5))\n","static_input = keras.Input(shape=(8))\n","#create empty list for modelresults\n","results = []\n","#define number of possible combination to try\n","for i in range(200):\n","    #choose random parameters\n","    units = random.choice(p_units)\n","    unitsMLP = random.choice(p_units2)\n","    BN = random.choice(p_BN)\n","    dropout = random.choice(p_dropout)\n","    BS = random.choice(p_BS)\n","    Class_weights = random.choice(p_cw)\n","\n","    #define LSTM:\n","\n","    LSTM_output = LSTM(units)(sensor_input)\n","    #add batchnormalisation depending on hyperparameter choice\n","    if BN == 1:\n","        BN_output = BatchNormalization()(LSTM_output)\n","    else: \n","        BN_output = LSTM_output\n","    #combine with static features\n","    all_features = concatenate([BN_output, static_input])\n","\n","    #define MLP layer \n","    MLP_output = Dense(unitsMLP, activation = 'relu')(all_features)\n","\n","    #add dropout\n","    Dropout_output = Dropout(dropout)(MLP_output)\n","\n","    #define final layer\n","    prediction =Dense(1, activation = 'sigmoid')(Dropout_output)\n","\n","    #define model\n","    Model = keras.Model(\n","    inputs = [sensor_input, static_input],\n","    outputs =prediction)\n","    #compile model\n","    Model.compile(optimizer = 'Adam', loss='binary_crossentropy',  metrics = ['AUC', 'Precision', 'Recall'])\n","    #fit model, add earlystopping and validation data \n","    Model.fit([x_train, x_static_lean], y_train, batch_size = BS, epochs=100, class_weight = Class_weights, validation_data=([x_val,x_static_val_lean], y_val), callbacks = [my_callbacks],  use_multiprocessing = True )\n","    #predict y-values validation set\n","    pred = Model.predict([x_val, x_static_val_lean])\n","    pred_clases = np.where(np.squeeze(pred) < 0.5, 0, 1) \n","    #append model results to list and the hyperparameters used to make the model \n","    results.append({\n","        'f1_score': f1_score(y_val, pred_clases),\n","        'AUC': roc_auc_score(y_val, pred), \n","        'Average Precision': average_precision_score(y_val, pred), \n","        'p_units': units,\n","        'p_units2': unitsMLP,\n","        'p_dropout': dropout,\n","        'p_BN' : BN,\n","        'p_BS' : BS,\n","        'p_cw' : Class_weights               \n","        })\n","#convert list to dataframe\n","results = pd.DataFrame(results)\n","#sort by AUC\n","results = results.sort_values(by='AUC', ascending=False)\n","display(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9eb29c9c-b2fe-4bda-bbd7-02a46eff8ce5","showTitle":true,"title":"func, static 8, up, y-set 1"}},"outputs":[],"source":["#hyperparameter tuning using random search for multi-input functional model\n","from keras.layers import Dense, LSTM, Dropout, BatchNormalization\n","import random\n","#define inputs\n","sensor_input = keras.Input(shape=(21,5))\n","static_input = keras.Input(shape=(8))\n","#create empty list for modelresults\n","results = []\n","#define number of possible combination to try\n","for i in range(200):\n","    #choose random parameters\n","    units = random.choice(p_units)\n","    unitsMLP = random.choice(p_units2)\n","    BN = random.choice(p_BN)\n","    dropout = random.choice(p_dropout)\n","    BS = random.choice(p_BS)\n","    Class_weights = random.choice(p_cw)\n","\n","    #define LSTM:\n","\n","    LSTM_output = LSTM(units)(sensor_input)\n","    #add batchnormalisation depending on hyperparameter choice\n","    if BN == 1:\n","        BN_output = BatchNormalization()(LSTM_output)\n","    else: \n","        BN_output = LSTM_output\n","    #combine with static features\n","    all_features = concatenate([BN_output, static_input])\n","\n","    #define MLP layer \n","    MLP_output = Dense(unitsMLP, activation = 'relu')(all_features)\n","\n","    #add dropout\n","    Dropout_output = Dropout(dropout)(MLP_output)\n","\n","    #define final layer\n","    prediction =Dense(1, activation = 'sigmoid')(Dropout_output)\n","\n","    #define model\n","    Model = keras.Model(\n","    inputs = [sensor_input, static_input],\n","    outputs =prediction)\n","    #compile model\n","    Model.compile(optimizer = 'Adam', loss='binary_crossentropy',  metrics = ['AUC', 'Precision', 'Recall'])\n","    #fit model, add earlystopping and validation data \n","    Model.fit([x_train_up, x_static_up_lean], y_train_up, batch_size = BS, epochs=100, class_weight = Class_weights, validation_data=([x_val,x_static_val_lean], y_val), callbacks = [my_callbacks],  use_multiprocessing = True )\n","    #predict y-values validation set\n","    pred = Model.predict([x_val, x_static_val_lean])\n","    pred_clases = np.where(np.squeeze(pred) < 0.5, 0, 1) \n","    #append model results to list and the hyperparameters used to make the model \n","    results.append({\n","        'f1_score': f1_score(y_val, pred_clases),\n","        'AUC': roc_auc_score(y_val, pred), \n","        'Average Precision': average_precision_score(y_val, pred), \n","        'p_units': units,\n","        'p_units2': unitsMLP,\n","        'p_dropout': dropout,\n","        'p_BN' : BN,\n","        'p_BS' : BS,\n","        'p_cw' : Class_weights               \n","        })\n","#convert list to dataframe\n","results = pd.DataFrame(results)\n","#sort by AUC\n","results = results.sort_values(by='AUC', ascending=False)\n","display(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"172977be-6102-4aa4-9f94-acc4cafa2ec4","showTitle":true,"title":"func, static 8, non up, y-set 2"}},"outputs":[],"source":["#hyperparameter tuning using random search for multi-input functional model\n","from keras.layers import Dense, LSTM, Dropout, BatchNormalization\n","import random\n","#define inputs\n","sensor_input = keras.Input(shape=(21,5))\n","static_input = keras.Input(shape=(8))\n","#create empty list for modelresults\n","results = []\n","#define number of possible combination to try\n","for i in range(200):\n","    #choose random parameters\n","    units = random.choice(p_units)\n","    unitsMLP = random.choice(p_units2)\n","    BN = random.choice(p_BN)\n","    dropout = random.choice(p_dropout)\n","    BS = random.choice(p_BS)\n","    Class_weights = random.choice(p_cw)\n","\n","    #define LSTM:\n","\n","    LSTM_output = LSTM(units)(sensor_input)\n","    #add batchnormalisation depending on hyperparameter choice\n","    if BN == 1:\n","        BN_output = BatchNormalization()(LSTM_output)\n","    else: \n","        BN_output = LSTM_output\n","    #combine with static features\n","    all_features = concatenate([BN_output, static_input])\n","\n","    #define MLP layer \n","    MLP_output = Dense(unitsMLP, activation = 'relu')(all_features)\n","\n","    #add dropout\n","    Dropout_output = Dropout(dropout)(MLP_output)\n","\n","    #define final layer\n","    prediction =Dense(1, activation = 'sigmoid')(Dropout_output)\n","\n","    #define model\n","    Model = keras.Model(\n","    inputs = [sensor_input, static_input],\n","    outputs =prediction)\n","    #compile model\n","    Model.compile(optimizer = 'Adam', loss='binary_crossentropy',  metrics = ['AUC', 'Precision', 'Recall'])\n","    #fit model, add earlystopping and validation data \n","    Model.fit([x_train, x_static_lean], y_train2, batch_size = BS, epochs=100, class_weight = Class_weights, validation_data=([x_val,x_static_val_lean], y_val2), callbacks = [my_callbacks],  use_multiprocessing = True, verbose = 0)\n","    #predict y-values validation set\n","    pred = Model.predict([x_val, x_static_val_lean])\n","    pred_clases = np.where(np.squeeze(pred) < 0.5, 0, 1) \n","    #append model results to list and the hyperparameters used to make the model \n","    results.append({\n","        'f1_score': f1_score(y_val2, pred_clases),\n","        'AUC': roc_auc_score(y_val2, pred), \n","        'Average Precision': average_precision_score(y_val2, pred), \n","        'p_units': units,\n","        'p_units2': unitsMLP,\n","        'p_dropout': dropout,\n","        'p_BN' : BN,\n","        'p_BS' : BS,\n","        'p_cw' : Class_weights               \n","        })\n","#convert list to dataframe\n","results = pd.DataFrame(results)\n","#sort by AUC\n","results = results.sort_values(by='AUC', ascending=False)\n","display(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"170d026a-39de-45a4-9abf-e511b5e8895a","showTitle":true,"title":"func, static 8, up, y-set 2"}},"outputs":[],"source":["#hyperparameter tuning using random search for multi-input functional model\n","from keras.layers import Dense, LSTM, Dropout, BatchNormalization\n","import random\n","#define inputs\n","sensor_input = keras.Input(shape=(21,5))\n","static_input = keras.Input(shape=(8))\n","#create empty list for modelresults\n","results = []\n","#define number of possible combination to try\n","for i in range(200):\n","    #choose random parameters\n","    units = random.choice(p_units)\n","    unitsMLP = random.choice(p_units2)\n","    BN = random.choice(p_BN)\n","    dropout = random.choice(p_dropout)\n","    BS = random.choice(p_BS)\n","    Class_weights = random.choice(p_cw)\n","\n","    #define LSTM:\n","\n","    LSTM_output = LSTM(units)(sensor_input)\n","    #add batchnormalisation depending on hyperparameter choice\n","    if BN == 1:\n","        BN_output = BatchNormalization()(LSTM_output)\n","    else: \n","        BN_output = LSTM_output\n","    #combine with static features\n","    all_features = concatenate([BN_output, static_input])\n","\n","    #define MLP layer \n","    MLP_output = Dense(unitsMLP, activation = 'relu')(all_features)\n","\n","    #add dropout\n","    Dropout_output = Dropout(dropout)(MLP_output)\n","\n","    #define final layer\n","    prediction =Dense(1, activation = 'sigmoid')(Dropout_output)\n","\n","    #define model\n","    Model = keras.Model(\n","    inputs = [sensor_input, static_input],\n","    outputs =prediction)\n","    #compile model\n","    Model.compile(optimizer = 'Adam', loss='binary_crossentropy',  metrics = ['AUC', 'Precision', 'Recall'])\n","    #fit model, add earlystopping and validation data \n","    Model.fit([x_train_up, x_static_up_lean], y_train2_up, batch_size = BS, epochs=100, class_weight = Class_weights, validation_data=([x_val,x_static_val_lean], y_val2), callbacks = [my_callbacks],  use_multiprocessing = True, verbose = 0)\n","    #predict y-values validation set\n","    pred = Model.predict([x_val, x_static_val_lean])\n","    pred_clases = np.where(np.squeeze(pred) < 0.5, 0, 1) \n","    #append model results to list and the hyperparameters used to make the model \n","    results.append({\n","        'f1_score': f1_score(y_val2, pred_clases),\n","        'AUC': roc_auc_score(y_val2, pred), \n","        'Average Precision': average_precision_score(y_val2, pred), \n","        'p_units': units,\n","        'p_units2': unitsMLP,\n","        'p_dropout': dropout,\n","        'p_BN' : BN,\n","        'p_BS' : BS,\n","        'p_cw' : Class_weights               \n","        })\n","#convert list to dataframe\n","results = pd.DataFrame(results)\n","#sort by AUC\n","results = results.sort_values(by='AUC', ascending=False)\n","display(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"64015033-37c1-4819-8b9d-172a792ce59a","showTitle":true,"title":"func, static 10, non up, y-set 1"}},"outputs":[],"source":["#hyperparameter tuning using random search for multi-input functional model\n","from keras.layers import Dense, LSTM, Dropout, BatchNormalization\n","import random\n","#define inputs\n","sensor_input = keras.Input(shape=(21,5))\n","static_input = keras.Input(shape=(10))\n","#create empty list for modelresults\n","results = []\n","#define number of possible combination to try\n","for i in range(200):\n","    #choose random parameters\n","    units = random.choice(p_units)\n","    unitsMLP = random.choice(p_units2)\n","    BN = random.choice(p_BN)\n","    dropout = random.choice(p_dropout)\n","    BS = random.choice(p_BS)\n","    Class_weights = random.choice(p_cw)\n","\n","    #define LSTM:\n","\n","    LSTM_output = LSTM(units)(sensor_input)\n","    #add batchnormalisation depending on hyperparameter choice\n","    if BN == 1:\n","        BN_output = BatchNormalization()(LSTM_output)\n","    else: \n","        BN_output = LSTM_output\n","    #combine with static features\n","    all_features = concatenate([BN_output, static_input])\n","\n","    #define MLP layer \n","    MLP_output = Dense(unitsMLP, activation = 'relu')(all_features)\n","\n","    #add dropout\n","    Dropout_output = Dropout(dropout)(MLP_output)\n","\n","    #define final layer\n","    prediction =Dense(1, activation = 'sigmoid')(Dropout_output)\n","\n","    #define model\n","    Model = keras.Model(\n","    inputs = [sensor_input, static_input],\n","    outputs =prediction)\n","    #compile model\n","    Model.compile(optimizer = 'Adam', loss='binary_crossentropy',  metrics = ['AUC', 'Precision', 'Recall'])\n","    #fit model, add earlystopping and validation data \n","    Model.fit([x_train, x_Static], y_train, batch_size = BS, epochs=100, class_weight = Class_weights, validation_data=([x_val,x_Static_val], y_val), callbacks = [my_callbacks],  use_multiprocessing = True, verbose = 0)\n","    #predict y-values validation set\n","    pred = Model.predict([x_val, x_Static_val])\n","    pred_clases = np.where(np.squeeze(pred) < 0.5, 0, 1) \n","    #append model results to list and the hyperparameters used to make the model \n","    results.append({\n","        'f1_score': f1_score(y_val, pred_clases),\n","        'AUC': roc_auc_score(y_val, pred), \n","        'Average Precision': average_precision_score(y_val, pred), \n","        'p_units': units,\n","        'p_units2': unitsMLP,\n","        'p_dropout': dropout,\n","        'p_BN' : BN,\n","        'p_BS' : BS,\n","        'p_cw' : Class_weights               \n","        })\n","#convert list to dataframe\n","results = pd.DataFrame(results)\n","#sort by AUC\n","results = results.sort_values(by='AUC', ascending=False)\n","display(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"00dc6971-339b-4ad1-8264-720f0b925adb","showTitle":true,"title":"func, static 10, non up, y-set 2"}},"outputs":[],"source":["#hyperparameter tuning using random search for multi-input functional model\n","from keras.layers import Dense, LSTM, Dropout, BatchNormalization\n","import random\n","#define inputs\n","sensor_input = keras.Input(shape=(21,5))\n","static_input = keras.Input(shape=(10))\n","#create empty list for modelresults\n","results = []\n","#define number of possible combination to try\n","for i in range(200):\n","    #choose random parameters\n","    units = random.choice(p_units)\n","    unitsMLP = random.choice(p_units2)\n","    BN = random.choice(p_BN)\n","    dropout = random.choice(p_dropout)\n","    BS = random.choice(p_BS)\n","    Class_weights = random.choice(p_cw)\n","\n","    #define LSTM:\n","\n","    LSTM_output = LSTM(units)(sensor_input)\n","    #add batchnormalisation depending on hyperparameter choice\n","    if BN == 1:\n","        BN_output = BatchNormalization()(LSTM_output)\n","    else: \n","        BN_output = LSTM_output\n","    #combine with static features\n","    all_features = concatenate([BN_output, static_input])\n","\n","    #define MLP layer \n","    MLP_output = Dense(unitsMLP, activation = 'relu')(all_features)\n","\n","    #add dropout\n","    Dropout_output = Dropout(dropout)(MLP_output)\n","\n","    #define final layer\n","    prediction =Dense(1, activation = 'sigmoid')(Dropout_output)\n","\n","    #define model\n","    Model = keras.Model(\n","    inputs = [sensor_input, static_input],\n","    outputs =prediction)\n","    #compile model\n","    Model.compile(optimizer = 'Adam', loss='binary_crossentropy',  metrics = ['AUC', 'Precision', 'Recall'])\n","    #fit model, add earlystopping and validation data \n","    Model.fit([x_train, x_Static], y_train2, batch_size = BS, epochs=100, class_weight = Class_weights, validation_data=([x_val,x_Static_val], y_val2), callbacks = [my_callbacks],  use_multiprocessing = True, verbose = 0)\n","    #predict y-values validation set\n","    pred = Model.predict([x_val, x_Static_val])\n","    pred_clases = np.where(np.squeeze(pred) < 0.5, 0, 1) \n","    #append model results to list and the hyperparameters used to make the model \n","    results.append({\n","        'f1_score': f1_score(y_val2, pred_clases),\n","        'AUC': roc_auc_score(y_val2, pred), \n","        'Average Precision': average_precision_score(y_val2, pred), \n","        'p_units': units,\n","        'p_units2': unitsMLP,\n","        'p_dropout': dropout,\n","        'p_BN' : BN,\n","        'p_BS' : BS,\n","        'p_cw' : Class_weights               \n","        })\n","#convert list to dataframe\n","results = pd.DataFrame(results)\n","#sort by AUC\n","results = results.sort_values(by='AUC', ascending=False)\n","display(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"84fde7ae-cc94-4757-8a69-3f26e02c43ad","showTitle":true,"title":"func, static 10, up, y-set 1"}},"outputs":[],"source":["#hyperparameter tuning using random search for multi-input functional model\n","from keras.layers import Dense, LSTM, Dropout, BatchNormalization\n","import random\n","#define inputs\n","sensor_input = keras.Input(shape=(21,5))\n","static_input = keras.Input(shape=(10))\n","#create empty list for modelresults\n","results = []\n","#define number of possible combination to try\n","for i in range(200):\n","    #choose random parameters\n","    units = random.choice(p_units)\n","    unitsMLP = random.choice(p_units2)\n","    BN = random.choice(p_BN)\n","    dropout = random.choice(p_dropout)\n","    BS = random.choice(p_BS)\n","    Class_weights = random.choice(p_cw)\n","\n","    #define LSTM:\n","\n","    LSTM_output = LSTM(units)(sensor_input)\n","    #add batchnormalisation depending on hyperparameter choice\n","    if BN == 1:\n","        BN_output = BatchNormalization()(LSTM_output)\n","    else: \n","        BN_output = LSTM_output\n","    #combine with static features\n","    all_features = concatenate([BN_output, static_input])\n","\n","    #define MLP layer \n","    MLP_output = Dense(unitsMLP, activation = 'relu')(all_features)\n","\n","    #add dropout\n","    Dropout_output = Dropout(dropout)(MLP_output)\n","\n","    #define final layer\n","    prediction =Dense(1, activation = 'sigmoid')(Dropout_output)\n","\n","    #define model\n","    Model = keras.Model(\n","    inputs = [sensor_input, static_input],\n","    outputs =prediction)\n","    #compile model\n","    Model.compile(optimizer = 'Adam', loss='binary_crossentropy',  metrics = ['AUC', 'Precision', 'Recall'])\n","    #fit model, add earlystopping and validation data \n","    Model.fit([x_train_up, x_Static_up], y_train_up, batch_size = BS, epochs=100, class_weight = Class_weights, validation_data=([x_val,x_Static_val], y_val), callbacks = [my_callbacks],  use_multiprocessing = True, verbose = 0)\n","    #predict y-values validation set\n","    pred = Model.predict([x_val, x_Static_val], verbose = 0)\n","    pred_clases = np.where(np.squeeze(pred) < 0.5, 0, 1) \n","    #append model results to list and the hyperparameters used to make the model \n","    results.append({\n","        'f1_score': f1_score(y_val, pred_clases),\n","        'AUC': roc_auc_score(y_val, pred), \n","        'Average Precision': average_precision_score(y_val, pred), \n","        'p_units': units,\n","        'p_units2': unitsMLP,\n","        'p_dropout': dropout,\n","        'p_BN' : BN,\n","        'p_BS' : BS,\n","        'p_cw' : Class_weights               \n","        })\n","#convert list to dataframe\n","results = pd.DataFrame(results)\n","#sort by AUC\n","results = results.sort_values(by='AUC', ascending=False)\n","display(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9141c15b-a0d1-47a0-b62a-8ea5304eaa8c","showTitle":true,"title":"func, static 10, up, y-set 2"}},"outputs":[],"source":["#hyperparameter tuning using random search for multi-input functional model\n","from keras.layers import Dense, LSTM, Dropout, BatchNormalization\n","import random\n","#define inputs\n","sensor_input = keras.Input(shape=(21,5))\n","static_input = keras.Input(shape=(10))\n","#create empty list for modelresults\n","results = []\n","#define number of possible combination to try\n","for i in range(200):\n","    #choose random parameters\n","    units = random.choice(p_units)\n","    unitsMLP = random.choice(p_units2)\n","    BN = random.choice(p_BN)\n","    dropout = random.choice(p_dropout)\n","    BS = random.choice(p_BS)\n","    Class_weights = random.choice(p_cw)\n","\n","    #define LSTM:\n","\n","    LSTM_output = LSTM(units)(sensor_input)\n","    #add batchnormalisation depending on hyperparameter choice\n","    if BN == 1:\n","        BN_output = BatchNormalization()(LSTM_output)\n","    else: \n","        BN_output = LSTM_output\n","    #combine with static features\n","    all_features = concatenate([BN_output, static_input])\n","\n","    #define MLP layer \n","    MLP_output = Dense(unitsMLP, activation = 'relu')(all_features)\n","\n","    #add dropout\n","    Dropout_output = Dropout(dropout)(MLP_output)\n","\n","    #define final layer\n","    prediction =Dense(1, activation = 'sigmoid')(Dropout_output)\n","\n","    #define model\n","    Model = keras.Model(\n","    inputs = [sensor_input, static_input],\n","    outputs =prediction)\n","    #compile model\n","    Model.compile(optimizer = 'Adam', loss='binary_crossentropy',  metrics = ['AUC', 'Precision', 'Recall'])\n","    #fit model, add earlystopping and validation data \n","    Model.fit([x_train_up, x_Static_up], y_train2_up, batch_size = BS, epochs=100, class_weight = Class_weights, validation_data=([x_val,x_Static_val], y_val2), callbacks = [my_callbacks],  use_multiprocessing = True, verbose = 0)\n","    #predict y-values validation set\n","    pred = Model.predict([x_val, x_Static_val], verbose = 0)\n","    pred_clases = np.where(np.squeeze(pred) < 0.5, 0, 1) \n","    #append model results to list and the hyperparameters used to make the model \n","    results.append({\n","        'f1_score': f1_score(y_val2, pred_clases),\n","        'AUC': roc_auc_score(y_val2, pred), \n","        'Average Precision': average_precision_score(y_val2, pred), \n","        'p_units': units,\n","        'p_units2': unitsMLP,\n","        'p_dropout': dropout,\n","        'p_BN' : BN,\n","        'p_BS' : BS,\n","        'p_cw' : Class_weights               \n","        })\n","#convert list to dataframe\n","results = pd.DataFrame(results)\n","#sort by AUC\n","results = results.sort_values(by='AUC', ascending=False)\n","display(results)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Feature importance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#test jaccard_score > supposed to be a good metric for performance for a binary classification mode, but first chose cut-off  \n","from sklearn.metrics import mean_squared_error, jaccard_score, accuracy_score\n","#make prediction\n","pred = Model.predict([x_val, x_static_val_lean])\n","#convert output to 0 or 1 \n","pred_clases = np.where(np.squeeze(pred) < 0.3, 0, 1)\n","#calculate performance metrics \n","jaccard = jaccard_score(y_val2, pred_clases)\n","auc = roc_auc_score(y_val2, pred) \n","accuracy = accuracy_score(y_val2, pred_clases)\n","print('jaccard =', jaccard, 'auc =', auc, 'accuracy =', accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#same as previous cell but for sequential model \n","#make prediction\n","pred = model.predict(x_val)\n","# convert to 0 or 1 \n","pred_clases = np.where(np.squeeze(pred) < 0.6, 0, 1)\n","#performce metrics \n","jaccard = jaccard_score(y_val2, pred_clases)\n","auc = roc_auc_score(y_val2, pred) \n","print('jaccard =', jaccard, 'auc =', auc)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Feature importance with static "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.utils import shuffle\n","from numpy import random\n","from sklearn.metrics import mean_squared_error, jaccard_score\n","\n","sat_f_names = [\"Autumn\", \"Spring\", \"Summer\", \"Winter\", \"Parity1\", \"Parity2\", \"Parity3+\",\"CalciumDaysInMilk\"]\n","\n","#shuffle sensor features\n","def feature_importance(model, x_sensor, x_static, y, threshold, len_shuf):\n","\n","   \n","    \n","    #required length of bootstrap \n","    len_val = x_sensor.shape[2]\n","    \n","    #container\n","    performance_container = []\n","    \n","    #performance full (normal) model\n","    auc_full_mod = roc_auc_score(y,model.predict([x_sensor, x_static],verbose = 0))\n","    pred_clases_full = np.where(np.squeeze(model.predict([x_sensor, x_static],verbose = 0)) < threshold, 0, 1)\n","    jaccard_full_mod = jaccard_score(y,pred_clases_full)\n","    accuracy_full_mod = accuracy_score(y,pred_clases_full) \n","    \n","    \n","    \n","    for i in range(len_val):\n","        for j in range(len_shuf):\n","            #set column to 0\n","            x_cop = x_sensor.copy()\n","            random.shuffle(x_cop[:,:,i])\n","            # x_cop[:,:,i] = 0\n","            #predict using alternative set \n","            pred = model.predict([x_cop, x_static],verbose = 0)\n","            # pred classes based on threshold\n","            pred_clases = np.where(np.squeeze(pred) < threshold, 0, 1)\n","            #get performance metric\n","            auc = roc_auc_score(y, pred) \n","            jaccard = jaccard_score(y,pred_clases)\n","            accuracy = accuracy_score(y,pred_clases)\n","            # MSE = mean_squared_error(y, pred, squared = False)\n","            #average_precision = average_precision_score(y, pred)\n","            \n","            # print(\"feat %i sim %i\" % (i,j))\n","            #add to container \n","            performance_container.append({\"pred\": feature_names[i], \"delta_auc\": auc_full_mod - auc, \"delta_jaccard\": jaccard_full_mod - jaccard, 'delta_accuracy': accuracy_full_mod - accuracy})\n","        \n","    return(performance_container)\n","\n","#shuffle static features    \n","def feature_importance_static(model, x_sensor, x_static, y, threshold, len_shuf):\n","\n","   \n","    \n","    #required length of bootstrap \n","    len_val = x_static.shape[1]\n","    \n","    #container\n","    performance_container = []\n","    \n","    #performance full (normal) model\n","    auc_full_mod = roc_auc_score(y,model.predict([x_sensor, x_static],verbose = 0))\n","    pred_clases_full = np.where(np.squeeze(model.predict([x_sensor, x_static],verbose = 0)) < threshold, 0, 1)\n","    jaccard_full_mod = jaccard_score(y,pred_clases_full)\n","    accuracy_full_mod = accuracy_score(y,pred_clases_full) \n","    \n","    \n","    for i in range(len_val):\n","        for j in range(len_shuf):\n","            #set column to 0\n","            \n","            x_cop = x_static.copy()\n","            random.shuffle(x_cop[:,i])\n","            # x_cop[:,:,i] = 0\n","            #predict using alternative set \n","            pred = model.predict([x_sensor, x_cop],verbose = 0)\n","            #predict classes based on threshold\n","            pred_clases = np.where(np.squeeze(pred) < threshold, 0, 1)\n","            #get performance metric\n","            auc = roc_auc_score(y, pred) \n","            jaccard = jaccard_score(y,pred_clases)\n","            accuracy = accuracy_score(y,pred_clases)\n","            # MSE = mean_squared_error(y, pred, squared = False)\n","            #average_precision = average_precision_score(y, pred)\n","            \n","            # print(\"feat %i sim %i\" % (i,j))\n","            #add to container \n","            performance_container.append({\"pred\": sat_f_names[i], \"delta_auc\": auc_full_mod - auc, \"delta_jaccard\": jaccard_full_mod - jaccard, 'delta_accuracy': accuracy_full_mod - accuracy})\n","        \n","    return(performance_container)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Feature importance sequantial models without static features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.utils import shuffle\n","from numpy import random\n","from sklearn.metrics import mean_squared_error, jaccard_score\n","\n","def feature_importance_no_static(model, x_sensor, y, threshold, len_shuf):\n","\n","   \n","    \n","    #required length of bootstrap \n","    len_val = x_sensor.shape[2]\n","    \n","    #container\n","    performance_container = []\n","    \n","    #performance full (normal) model\n","    auc_full_mod = roc_auc_score(y,model.predict(x_sensor,verbose = 0))\n","    pred_clases_full = np.where(np.squeeze(model.predict(x_sensor,verbose = 0)) < threshold, 0, 1)\n","    jaccard_full_mod = jaccard_score(y,pred_clases_full)\n","    accuracy_full_mod = accuracy_score(y,pred_clases_full)\n","    \n","    \n","    \n","    for i in range(len_val):\n","        for j in range(len_shuf):\n","            #set column to 0\n","            x_cop = x_sensor.copy()\n","            random.shuffle(x_cop[:,:,i])\n","            # x_cop[:,:,i] = 0\n","            #predict using alternative set \n","            pred = model.predict(x_cop, verbose = 0)\n","            # pred classes based on threshold\n","            pred_clases = np.where(np.squeeze(pred) < threshold, 0, 1)\n","            #get performance metric\n","            auc = roc_auc_score(y, pred) \n","            jaccard = jaccard_score(y,pred_clases)\n","            accuracy = accuracy_score(y,pred_clases)\n","            #average_precision = average_precision_score(y, pred)\n","            \n","            # print(\"feat %i sim %i\" % (i,j))\n","            #add to container \n","            performance_container.append({\"pred\": feature_names[i], \"delta_auc\": auc_full_mod - auc, \"delta_jaccard\": jaccard_full_mod - jaccard, 'delta_accuracy': accuracy_full_mod - accuracy})\n","        \n","    return(performance_container)\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#feature importance for the model without static features \n","result_feature_no_static = feature_importance_no_static(model, x_val, y_val2, 0.6, 50)\n","df_result_feature_no_static = pd.DataFrame(result_feature_no_static).groupby('pred').mean().sort_values(by = 'delta_accuracy', ascending = False)\n","df_result_feature_no_static "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#feature importance per static feature \n","result_perf_test_static = feature_importance_static(Model, x_val, x_static_val_lean, y_val2,0.3,50)\n","df_results_perf_stat = pd.DataFrame(result_perf_test_static).groupby('pred').mean().sort_values(by = 'delta_accuracy', ascending = False)\n","df_results_perf_stat \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#feature importances per behaviour\n","result_perf_test = feature_importance(Model, x_val, x_static_val_lean, y_val2, 0.2, 50)\n","df_results_perf = pd.DataFrame(result_perf_test).groupby('pred').mean().sort_values(by = 'delta_accuracy',\n","                                              ascending = False)\n","df_results_perf "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Alternative feature importance whereby all values of a feature are converted to 0 and then performance metrics of the model are calculated\n","x_t_sens = np.zeros(x_val.shape)\n","#select columns no to convert to 0\n","x_t_sens[:,:,[11,17,14,18,16,8,5,10,12,3,0,7,2,13,6,1,15,4,9]] = x_val[:,:,[11,17,14,18,16,8,5,10,12,3,0,7,2,13,6,1,15,4,9]]\n","x_t_sens_static = np.zeros(x_static_val_lean.shape)\n","#predict\n","pred = Model.predict([x_val, x_t_sens_static])\n","auc = roc_auc_score(y_val2, pred) \n","auc"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"Calciumclustermodel","notebookOrigID":2393132451168081,"widgets":{}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
