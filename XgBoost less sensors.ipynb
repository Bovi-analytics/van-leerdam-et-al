{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reruning the models without day 0. Day 0 turns out to have a lot of missing data and I found it therefore to be worth the effort to check whether the models would improve if day 0 was removed. Script for the XgBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data sets\n",
    "import pandas as pd\n",
    "Imputed_test_set = pd.read_csv(\"Data/Imputed_test_set.csv\")\n",
    "Imputed_train_set = pd.read_csv(\"Data/Imputed_train_set.csv\")\n",
    "Imputed_train_set_up = pd.read_csv(\"Data/Imputed_train_set_up.csv\")\n",
    "Imputed_validation_set = pd.read_csv(\"Data/Imputed_validation_set.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting x and y-values as model input. Sequential sensordata is stored in a 3d matrix of 21 days per 5 features per 365 cows.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#extracting all unique combinations of cow and calving moment \n",
    "Unique_Calvings = Imputed_train_set[['AnimalEartag', 'PaperRecordedCalvingDate']].drop_duplicates() \n",
    "\n",
    "#define sensors\n",
    "feature_names = [\"WalkingTimeMinutesPerDay\", \"EatingTimeMinutesPerDay\", \"LyingTimeMinutesPerDay\", \"StandingTimeMinutesPerDay\", \"RuminationTimeMinutesPerDay\"]\n",
    "\n",
    "#create empty lists per feature to store variables\n",
    "AnimalEartag_list = [] \n",
    "PaperRecordedCalvingDate_list = []\n",
    "CalciumValue_list = []\n",
    "Calciumcluster_list = []\n",
    "BCS_lijst = []\n",
    "Loco_lijst = []\n",
    "SensorWaardes_list = np.zeros((365,21,5)) #define matrix size for sequential features\n",
    "j = 0\n",
    "\n",
    "#itterating through the dataset in order to extract x and y for each cow \n",
    "for index, (AnimalEartag, PaperRecordedCalvingDate) in Unique_Calvings.iterrows():\n",
    "    filter1 = Imputed_train_set['AnimalEartag'] == AnimalEartag\n",
    "    filter2 = PaperRecordedCalvingDate == Imputed_train_set['PaperRecordedCalvingDate']\n",
    "    df_calving = Imputed_train_set[filter1 & filter2]\n",
    "    Loco = df_calving['FirstLocomotionScore'].max()\n",
    "    BCS =  df_calving['FirstBCSScore'].max()\n",
    "    cacluster = df_calving['Calciumcluster'].iloc[-1] \n",
    "    ca = df_calving['Cut_Off'].iloc[-1]\n",
    "    sw = df_calving[feature_names]\n",
    "   \n",
    "    \n",
    "  \n",
    "    #convert to numpy\n",
    "    sw_numpy = np.array(sw)\n",
    "    #add to list \n",
    "    AnimalEartag_list.append(AnimalEartag)\n",
    "    PaperRecordedCalvingDate_list.append(PaperRecordedCalvingDate)\n",
    "    CalciumValue_list.append(ca)\n",
    "    Calciumcluster_list.append(cacluster)\n",
    "    BCS_lijst.append((BCS))\n",
    "    Loco_lijst.append((Loco))\n",
    "    \n",
    "   \n",
    "    SensorWaardes_list[j] = sw_numpy\n",
    "    j = j + 1\n",
    " \n",
    "#convert to numpy arrays\n",
    "x_train = np.array(SensorWaardes_list)\n",
    "y_train = np.asarray(Calciumcluster_list) #based on cluster method \n",
    "y_train2 = np.asarray(CalciumValue_list) #alternative method of categorisation based on cut-off value\n",
    "x_BCS = np.asarray(BCS_lijst)\n",
    "x_Loco = np.asarray(Loco_lijst)\n",
    "x_static = np.stack((x_BCS, x_Loco), axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONEHOTENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calvingseason and parity are cathegorial variables, however the models functions on numerical variables only, therefore the variables are converted into binairy variables using the sklearn onehotencoder. Each category gets its own colummn and the column of the category that is true gets a one, all other get a zero.     \n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#extract value for each cow\n",
    "static = Imputed_train_set.groupby(['AnimalEartag', 'PaperRecordedCalvingDate']).first()\n",
    "#convert to numpy\n",
    "season = static['CalvingSeason'].to_numpy()\n",
    "Parity = static['Parity'].to_numpy()\n",
    "CalciumDaysInMilk = static['CalciumDaysInMilk'].to_numpy()\n",
    "#define labelencoder\n",
    "labelEnc = LabelEncoder()\n",
    "#fit and apply\n",
    "x_enc = labelEnc.fit_transform(season)\n",
    "x_enc = x_enc.reshape(len(x_enc), 1) \n",
    "onehotEnc = OneHotEncoder(sparse=False)\n",
    "season_encoded = onehotEnc.fit_transform(x_enc)\n",
    "x_enc_P = labelEnc.fit_transform(Parity)\n",
    "x_enc_P = x_enc_P.reshape(len(x_enc_P), 1) \n",
    "parity_encoded = onehotEnc.fit_transform(x_enc_P)\n",
    "#combine season, parity and day of measurement into one numpy array \n",
    "x_static_lean = np.column_stack([season_encoded, parity_encoded, CalciumDaysInMilk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BCS and Locomotion score was not measured for every cow, therefore some cows have a null value. The null values are replaced by the value that is most frequent in that colummn. \n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_freq = SimpleImputer(missing_values=0.0, strategy='most_frequent')\n",
    "imp_freq.fit(x_static)\n",
    "x_static = imp_freq.transform(x_static)\n",
    "#all static features are added up into one numpy array \n",
    "x_Static = np.column_stack([x_static, season_encoded, parity_encoded, CalciumDaysInMilk]) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsampled train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since this train set is upsampled, it is impossible to filter on animal eartag and paperrecordedcalvingdate (some cows of cathegory 1 are duplicated and therefore no longer unique) therefore a new column was introduced while performing upsampling called 'Samplenumber' In order to extract an x and an y per sample, an itteration is performed based on samplenumber. \n",
    "Unique_Calvings = Imputed_train_set_up['SampleNumber'].drop_duplicates()\n",
    "\n",
    "\n",
    "#creating empty lists, pay attention to the increased matrix size and the addition of a list for samplenumber\n",
    "SampleNumber_list = []\n",
    "CalciumValue_list = []\n",
    "Calciumcluster_list = []\n",
    "SensorWaardes_list = np.zeros((534,21,5)) #534 cows, 21 days, 5 sensors\n",
    "BCS_lijst = []\n",
    "Loco_lijst = []\n",
    "j = 0\n",
    "for s in range(len(Unique_Calvings)):\n",
    "    filter = Imputed_train_set_up['SampleNumber'] == s\n",
    "    df_calving = Imputed_train_set_up[filter]\n",
    "    Loco = df_calving['FirstLocomotionScore'].max()\n",
    "    BCS =  df_calving['FirstBCSScore'].max()\n",
    "    i = 1\n",
    "    cacluster = df_calving['Calciumcluster'].iloc[-i]\n",
    "    ca = df_calving['Cut_Off'].iloc[-i]\n",
    "    i = i + 1 #prevents infinite loop\n",
    "  \n",
    "    sw = df_calving[feature_names]\n",
    "  \n",
    "    #convert to numpy\n",
    "    sw_numpy = np.array(sw)\n",
    "    #add to list\n",
    "    SampleNumber_list.append(s)\n",
    "    CalciumValue_list.append(ca)\n",
    "    Calciumcluster_list.append(cacluster)\n",
    "    BCS_lijst.append((BCS))\n",
    "    Loco_lijst.append((Loco))\n",
    "  \n",
    "    SensorWaardes_list[j] = sw_numpy\n",
    "    j = j + 1\n",
    "\n",
    "#transform to numpy array\n",
    "x_train_up = np.array(SensorWaardes_list)\n",
    "y_train_up = np.asarray(Calciumcluster_list)\n",
    "y_train2_up =np.asarray(CalciumValue_list)\n",
    "x_BCS_up = np.asarray(BCS_lijst) #BCS end dry period \n",
    "x_Loco_up = np.asarray(Loco_lijst) #locomotionscore end dry period\n",
    "x_static_up = np.stack((x_BCS_up, x_Loco_up), axis = 1) #combining BCS and Locomotion in order to be able to perform upsampling efficiently"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one hot encoder upsampled train set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding upsampled train set\n",
    "#Calvingseason and parity are cathegorial variables, however the models functions on numerical variables only, therefore the variables are converted into binairy variables using the sklearn onehotencoder. Each category gets its own colummn and the column of the category that is true gets a one, all other get a zero.\n",
    "#based on samplenumber instead of animaleartag due to the upsampling \n",
    "static = Imputed_train_set_up.groupby(['SampleNumber']).first()\n",
    "#convert to numpy arrays\n",
    "season = static['CalvingSeason'].to_numpy()\n",
    "Parity = static['Parity'].to_numpy()\n",
    "CalciumDaysInMilk_up = static['CalciumDaysInMilk'].to_numpy()\n",
    "#define encoder\n",
    "labelEnc = LabelEncoder()\n",
    "#fit and apply\n",
    "x_enc = labelEnc.fit_transform(season)\n",
    "x_enc = x_enc.reshape(len(x_enc), 1) \n",
    "onehotEnc = OneHotEncoder(sparse=False)\n",
    "season_encoded_up = onehotEnc.fit_transform(x_enc)\n",
    "x_enc_P = labelEnc.fit_transform(Parity)\n",
    "x_enc_P = x_enc_P.reshape(len(x_enc_P), 1) \n",
    "parity_encoded_up = onehotEnc.fit_transform(x_enc_P)\n",
    "x_static_up = imp_freq.transform(x_static_up)\n",
    "#combine parity, calving season and day of measurement\n",
    "x_static_up_lean = np.column_stack([season_encoded_up, parity_encoded_up, CalciumDaysInMilk_up])\n",
    "#combine all static features into one array\n",
    "x_Static_up = np.column_stack([x_static_up, season_encoded_up, parity_encoded_up, CalciumDaysInMilk_up]) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting all unique combinations of cow and calving moment  \n",
    "Unique_Calvings_Val = Imputed_validation_set[['AnimalEartag', 'PaperRecordedCalvingDate']].drop_duplicates()\n",
    "\n",
    "\n",
    "AnimalEartagV_list = []\n",
    "PaperRecordedCalvingDateV_list = []\n",
    "CalciumValueV_list = []\n",
    "CalciumclusterV_list = []\n",
    "BCS_lijst = []\n",
    "Loco_lijst = []\n",
    "SensorWaardesV_list = np.zeros((122,21,5)) #122 cows, 21 days, 5 sensors\n",
    "j = 0\n",
    "for index, (AnimalEartag, PaperRecordedCalvingDate) in Unique_Calvings_Val.iterrows():\n",
    "    filter1 = Imputed_validation_set['AnimalEartag'] == AnimalEartag\n",
    "    filter2 = PaperRecordedCalvingDate == Imputed_validation_set['PaperRecordedCalvingDate']\n",
    "    df_calving_val = Imputed_validation_set[filter1 & filter2]\n",
    "    i = 1\n",
    "    Loco = df_calving_val['FirstLocomotionScore'].max()\n",
    "    BCS =  df_calving_val['FirstBCSScore'].max()\n",
    "    caV = df_calving_val['Cut_Off'].iloc[-i]\n",
    "    caclusterV = df_calving_val['Calciumcluster'].iloc[-i]\n",
    "    swV = df_calving_val[feature_names]\n",
    "  \n",
    "    #convert to numpy\n",
    "    swV_numpy = np.array(swV)\n",
    "    #add to list \n",
    "    AnimalEartagV_list.append(AnimalEartag)\n",
    "    PaperRecordedCalvingDateV_list.append(PaperRecordedCalvingDate)\n",
    "    CalciumValueV_list.append(caV)\n",
    "    CalciumclusterV_list.append(caclusterV)\n",
    "    BCS_lijst.append((BCS))\n",
    "    Loco_lijst.append((Loco))\n",
    "    \n",
    "    SensorWaardesV_list[j] = swV_numpy\n",
    "    j = j + 1\n",
    "\n",
    "#convert to numpy \n",
    "x_val = np.array(SensorWaardesV_list)\n",
    "y_val2 = np.asarray(CalciumValueV_list)\n",
    "y_val = np.asarray(CalciumclusterV_list)\n",
    "x_BCS = np.asarray(BCS_lijst)\n",
    "x_Loco = np.asarray(Loco_lijst)\n",
    "#stack BCS en locomotion score \n",
    "x_static_val = np.stack((x_BCS, x_Loco), axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onehotencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding validation\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "static = Imputed_validation_set.groupby(['AnimalEartag', 'PaperRecordedCalvingDate']).first()\n",
    "season = static['CalvingSeason'].to_numpy()\n",
    "Parity = static['Parity'].to_numpy()\n",
    "CalciumDaysInMilk = static['CalciumDaysInMilk'].to_numpy()\n",
    "labelEnc = LabelEncoder()\n",
    "x_enc = labelEnc.fit_transform(season)\n",
    "x_enc = x_enc.reshape(len(x_enc), 1) \n",
    "onehotEnc = OneHotEncoder(sparse=False)\n",
    "season_encoded = onehotEnc.fit_transform(x_enc)\n",
    "x_enc_P = labelEnc.fit_transform(Parity)\n",
    "x_enc_P = x_enc_P.reshape(len(x_enc_P), 1) \n",
    "parity_encoded = onehotEnc.fit_transform(x_enc_P)\n",
    "#combine static features \n",
    "x_static_val_lean = np.column_stack([season_encoded, parity_encoded, CalciumDaysInMilk])\n",
    "#impute missing BCS and Locomotion scores \n",
    "x_static_val = imp_freq.transform(x_static_val)\n",
    "# combine BCS and Locomotion scores with the other static features\n",
    "x_Static_val = np.column_stack([x_static_val, season_encoded, parity_encoded,CalciumDaysInMilk]) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting all unique combinations of cow and calving moment\n",
    "Unique_Calvings_Test = Imputed_test_set[['AnimalEartag', 'PaperRecordedCalvingDate']].drop_duplicates()\n",
    "\n",
    "AnimalEartag_Test_list = []\n",
    "PaperRecordedCalvingDate_Test_list = []\n",
    "CalciumValue_Test_list = []\n",
    "Calciumcluster_Test_list = []\n",
    "BCS_lijst = []\n",
    "Loco_lijst = []\n",
    "SensorWaardes_Test_list = np.zeros((122,21,5))\n",
    "k = 0\n",
    "for index, (AnimalEartag, PaperRecordedCalvingDate) in Unique_Calvings_Test.iterrows():\n",
    "    filter1 = Imputed_test_set['AnimalEartag'] == AnimalEartag\n",
    "    filter2 = PaperRecordedCalvingDate == Imputed_test_set['PaperRecordedCalvingDate']\n",
    "    df_calving_test = Imputed_test_set[filter1 & filter2]\n",
    "    Loco = df_calving_test['FirstLocomotionScore'].max()\n",
    "    BCS =  df_calving_test['FirstBCSScore'].max()\n",
    "    i = 1\n",
    "    catest = df_calving_test['Cut_Off'].iloc[-i]\n",
    "    caclustertest = df_calving_test['Calciumcluster'].iloc[-i]\n",
    "    a = df_calving_test['CalvingSeason'].dropna()\n",
    "  \n",
    "    swtest = df_calving_test[feature_names]\n",
    "\n",
    "    #convert to numpy\n",
    "    swtest_numpy = np.array(swtest)\n",
    "\n",
    "    #add to list \n",
    "    AnimalEartag_Test_list.append(AnimalEartag)\n",
    "    PaperRecordedCalvingDate_Test_list.append(PaperRecordedCalvingDate)\n",
    "    CalciumValue_Test_list.append(catest)\n",
    "    Calciumcluster_Test_list.append(caclustertest)\n",
    "    BCS_lijst.append((BCS))\n",
    "    Loco_lijst.append((Loco))\n",
    "    SensorWaardes_Test_list[k] = swtest_numpy\n",
    "    k = k + 1\n",
    "\n",
    "\n",
    "x_test = np.asarray(SensorWaardes_Test_list)\n",
    "y_test2 = np.asarray(CalciumValue_Test_list)\n",
    "y_test = np.asarray(Calciumcluster_Test_list)\n",
    "x_BCS = np.asarray(BCS_lijst)\n",
    "x_Loco = np.asarray(Loco_lijst)\n",
    "x_static_test = np.stack((x_BCS, x_Loco), axis = 1)\n",
    "#impute missing BCS and Locomotion scores\n",
    "x_static_test = imp_freq.transform(x_static_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding test set\n",
    "static = Imputed_test_set.groupby(['AnimalEartag', 'PaperRecordedCalvingDate']).first()\n",
    "season = static['CalvingSeason'].to_numpy()\n",
    "Parity = static['Parity'].to_numpy()\n",
    "CalciumDaysInMilk = static['CalciumDaysInMilk'].to_numpy()\n",
    "labelEnc = LabelEncoder()\n",
    "x_enc = labelEnc.fit_transform(season)\n",
    "x_enc = x_enc.reshape(len(x_enc), 1) \n",
    "onehotEnc = OneHotEncoder(sparse=False)\n",
    "season_encoded = onehotEnc.fit_transform(x_enc)\n",
    "x_enc_P = labelEnc.fit_transform(Parity)\n",
    "x_enc_P = x_enc_P.reshape(len(x_enc_P), 1) \n",
    "parity_encoded = onehotEnc.fit_transform(x_enc_P)\n",
    "#combine static features\n",
    "x_static_test_lean = np.column_stack([season_encoded, parity_encoded, CalciumDaysInMilk])\n",
    "# combine static features with BCS and Locomotion scores\n",
    "x_Static_test = np.column_stack([x_static_test, season_encoded, parity_encoded, CalciumDaysInMilk]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOOTSTRAPPEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XgBoost and Deep learning models have a form of randomness in their initialisation. The exact same model structure with the same training data may therefore give different results each time it is run. In order to be able to compare different models, model performance is not objective enough since it can differ with the same model and could be a high value simply because you were lucky. To compansate for this behaviour a second metric is proposed to compare models; the variance of the model. When a model has high variance, the chances are higher that a high value is obtained by luck and it could be that the second time the model is run, the exact same model results in dramaticly low performance metrics. To be able to measure varriance bootstraps are built, bootstraps are samples from the validation set with the same size as the validation set but acquired with sampling with replacement. Therefore the same model can be tested on multiple validation sets and the results can be compared and the SD calculated. \n",
    "\n",
    "#define function for creating bootstraps\n",
    "def create_bootstrap(x_sensor,x_static,y1, y2):\n",
    "    #initialise empty list for bootstraps\n",
    "    bootstrap_x_sensor = []\n",
    "    bootstrap_x_static = []\n",
    "    bootstrap_y1 = []\n",
    "    bootstrap_y2 = []\n",
    "    \n",
    "    #required length of bootstrap \n",
    "    len_val = x_val.shape[0]\n",
    "    \n",
    "    #get random observation \n",
    "    for i in range(len_val):\n",
    "        # get random index\n",
    "        random_idx = np.random.choice(range(len_val), 1)\n",
    "\t\t# get random observation\n",
    "        random_x_sensor = x_sensor[random_idx]\n",
    "        random_x_static = x_static[random_idx]\n",
    "        random_y1 = y1[random_idx]\n",
    "        random_y2 = y2[random_idx]\n",
    "        \n",
    "\t\t# add random observation to bootstrap\n",
    "        bootstrap_x_sensor.append(random_x_sensor)\n",
    "        bootstrap_x_static.append(random_x_static)\n",
    "        bootstrap_y1.append(random_y1)\n",
    "        bootstrap_y2.append(random_y2)\n",
    "        \n",
    "\t# convert to numpy\n",
    "    bootstrap_x_sensor = np.asarray(bootstrap_x_sensor) \n",
    "    bootstrap_x_static = np.asarray(bootstrap_x_static)\n",
    "    bootstrap_y1 = np.asarray(bootstrap_y1)\n",
    "    bootstrap_y2 = np.asarray(bootstrap_y2)\n",
    "\n",
    "\t# return\t\n",
    "    return(bootstrap_x_sensor, bootstrap_x_static, bootstrap_y1, bootstrap_y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to create bootstraps\n",
    "def create_bootstraps(x_sensor,x_static,y1, y2, number_bootstraps):\n",
    "\t\n",
    "# initialize bootstrap containers\n",
    "    bootstrap_container_x_sensor = []\n",
    "    bootstrap_container_x_static = []\n",
    "    bootstrap_container_y1 = []\n",
    "    bootstrap_container_y2 = []\n",
    "\t\t\n",
    "\t# create n bootstrap\n",
    "    for i in range(number_bootstraps):\n",
    "\t\t# get bootstrap\n",
    "        bootstrap_x_sensor, bootstrap_x_static, bootstrap_y1, bootstrap_y2 = create_bootstrap(x_sensor,x_static,y1, y2)\n",
    "\t\t# add to container\n",
    "        bootstrap_container_x_sensor.append(bootstrap_x_sensor)\n",
    "        bootstrap_container_x_static.append(bootstrap_x_static)\n",
    "        bootstrap_container_y1.append(bootstrap_y1)\n",
    "        bootstrap_container_y2.append(bootstrap_y2)\n",
    "\n",
    "\t# return\n",
    "    return(bootstrap_container_x_sensor, bootstrap_container_x_static, bootstrap_container_y1, bootstrap_container_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to evaluate model \n",
    "def evaluate_model(model, bootstrap_container_x_sensor, bootstrap_container_x_static, bootstrap_container_y1, bootstrap_container_y2):\n",
    "\n",
    "\t# initialize evaluation container\n",
    "    performance_container = []\n",
    "\n",
    "\t# loop through bootstraps\n",
    "    for i in range(len(bootstrap_container_x_sensor)):\n",
    "\n",
    "\t\t# get X\n",
    "        bootstrap_x_sensor = bootstrap_container_x_sensor[i]\n",
    "        bootstrap_x_static = bootstrap_container_x_static[i]\n",
    "\t\t# get y\n",
    "        bootstrap_y1 = bootstrap_container_y1[i]\n",
    "        bootstrap_y2 = bootstrap_container_y2[i]\n",
    "        #reshape x from 3d to 2d for machine learning \n",
    "        bootstrap_x_sensor = bootstrap_x_sensor.reshape((bootstrap_x_sensor.shape[0], (bootstrap_x_sensor.shape[1]*bootstrap_x_sensor.shape[2])))\n",
    "        # bootstrap_x_static = bootstrap_x_static.reshape(bootstrap_x_static.shape[0], (bootstrap_x_static.shape[1]*bootstrap_x_static.shape[2]))\n",
    "        #for XgBoost convert to d-matrix, comment out when not using XgBoost!\n",
    "        bootstrap_x_sensor =  xgb.DMatrix(bootstrap_x_sensor)\n",
    "\t\t# get predictions #depending on model to evaluate, for some models static features need to be added, for log reg model do not use predict but predict_proba  \n",
    "        preds = model.predict(bootstrap_x_sensor)    \n",
    "\t\t# get metric # first choose witch y set to test 1 = clustered, 2 = cut-off\n",
    "        auc = roc_auc_score(bootstrap_y2, preds)\n",
    "        \n",
    "\t\t# add to container\n",
    "        performance_container.append(auc)\n",
    "      \n",
    "\t# return\n",
    "    return(performance_container)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XgBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten numpy array \n",
    "#necessairy beceause machine learning models cannot funtion on 3d data \n",
    "\n",
    "x_Train = x_train.reshape(x_train.shape[0], (x_train.shape[1]*x_train.shape[2]))\n",
    "x_Train_up = x_train_up.reshape(x_train_up.shape[0], (x_train_up.shape[1]*x_train_up.shape[2]))\n",
    "x_Test = x_test.reshape(x_test.shape[0], (x_test.shape[1]*x_test.shape[2]))\n",
    "x_Val = x_val.reshape(x_val.shape[0], (x_val.shape[1]*x_val.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add static features to the model imput \n",
    "x_train_sensor_static = np.column_stack([x_Train, x_static_lean])\n",
    "x_val_sensor_static = np.column_stack([x_Val, x_static_val_lean])\n",
    "x_test_sensor_static = np.column_stack([x_Test, x_static_test_lean])\n",
    "x_train_sensor_static_up = np.column_stack([x_Train_up, x_static_up_lean])\n",
    "x_train_plus = np.column_stack([x_Train, x_Static])\n",
    "x_val_plus = np.column_stack([x_Val, x_Static_val])\n",
    "x_test_plus = np.column_stack([x_Test, x_Static_test])\n",
    "x_train_plus_up = np.column_stack([x_Train_up, x_Static_up])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model architecture XgBoost\n",
    "import xgboost as xgb\n",
    "# binary target\n",
    "#convert to special XgBoost matrix\n",
    "dtrain = xgb.DMatrix(x_train_plus, label=y_train2)\n",
    "dtest = xgb.DMatrix(x_test_plus)\n",
    "dval =  xgb.DMatrix(x_val_plus, label=y_val2)\n",
    "#set parameters\n",
    "param = {'max_depth': 6, \"min_child_weight\": 3 , 'eta':0.036069208257146906, 'gamma': 0, 'objective': 'binary:logistic'}\n",
    "param['scale_pos_weight'] = 1\n",
    "# (len(y_train)-sum(y_train))/sum(y_train) #total number of positive samples devided by the total number of negative sampels, method to deal with class imbalance \n",
    "param['eval_metric'] = 'auc' \n",
    "param['seed']=32\n",
    "#evaluation data for early stopping\n",
    "evallist = [(dtrain, 'train'), (dval, 'validation')]\n",
    "# evallist = [(dtest, 'test')] #earlystopping nu maar met de testset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 100 #define/use when not using early stopping. Early stopping is the better option most of the time since it prevents overfitting on the train set.  \n",
    "boost = xgb.train(param, dtrain, early_stopping_rounds = 200, evals = evallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bootstrapping\n",
    "#create 50 bootstraps for machine learning (different due to flattened x-values)\n",
    "Bootstrap_x_sensor_flat, Bootstrap_x_static, Bootstrap_y1, Bootstrap_y2  = create_bootstraps(x_val_plus, x_Static_val, y_val, y_val2, 50)\n",
    "Bootstrap_performance = evaluate_model(boost, Bootstrap_x_sensor_flat, Bootstrap_x_static, Bootstrap_y1, Bootstrap_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results bootstrapping, mean auc and std \n",
    "Mean_performance = np.mean(Bootstrap_performance)\n",
    "SD_performance = np.std(Bootstrap_performance)\n",
    "df = pd.DataFrame(Bootstrap_performance, columns = ['AUC'])\n",
    "# display(df)\n",
    "Mean_performance, SD_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict y-values of the test set\n",
    "ypred = boost.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to 0 or 1 value, threshold = 0.5\n",
    "pred_test_clases = np.where(np.squeeze(ypred) < 0.5, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare real y-values with predicted values using confusion matrix and classification report \n",
    "cmboost = confusion_matrix(y_test, pred_test_clases)\n",
    "dispboost = ConfusionMatrixDisplay(confusion_matrix=cmboost)\n",
    "dispboost.plot()\n",
    "print(classification_report(y_test, pred_test_clases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other evaluation metrics\n",
    "#Sensitivity, specificity and ppv threshold dependent, the default threshold is 0,5 \n",
    "#AUC and AP are threshold independent and therefore have to use the PROBABILITIES\n",
    "sensitiviteit = cmboost[1,1]/(cmboost[1,0]+cmboost[1,1])\n",
    "specificiteit = cmboost[0,0]/(cmboost[0,0]+cmboost[0,1])\n",
    "ppv = cmboost[1,1]/(cmboost[1,1]+cmboost[0,1])\n",
    "auc = roc_auc_score(y_test2, ypred)\n",
    "average_precision = average_precision_score(y_test2, ypred)\n",
    "print('sensitiviteit =', sensitiviteit, 'specificiteit =', specificiteit, 'ppv =', ppv, 'AUC =', auc, 'average_precision = ', average_precision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed for ray tuner\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from ray import tune\n",
    "#code for hyperparameter tuning XgBoost model using the Ray Tuner \n",
    "#while using databricks you need to manualy add ray to the cluster in the compute window, while adding select from pypi\n",
    "#define model\n",
    "\n",
    "def tree(config):\n",
    "    dtrain = xgb.DMatrix(x_train_sensor_static_up, label=y_train2_up)\n",
    "    dval =  xgb.DMatrix(x_val_sensor_static, label=y_val2)\n",
    "    results={}\n",
    "    bst = xgb.train(\n",
    "        config,\n",
    "        dtrain,\n",
    "        evals = [(dval, 'validation')],\n",
    "        evals_result=results,\n",
    "        verbose_eval=False,\n",
    "       ) \n",
    "    accuracy = 1. - results[\"validation\"][\"error\"][-1]\n",
    "    auc = results[\"validation\"][\"auc\"][-1]\n",
    "    \n",
    "    tune.report(mean_accuracy=accuracy, mean_auc = auc, done=True)\n",
    "                \n",
    "   \n",
    "#define hyperparameters you would like to tune \n",
    "if __name__ == \"__main__\":\n",
    "    config = { \n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"seed\": 32,\n",
    "        \"eval_metric\": [\"auc\", \"error\"],\n",
    "        'max_depth': tune.randint(2,10),\n",
    "        'min_child_weight' : tune.choice([1,2,3,4]),\n",
    "        'gamma' : tune.choice([0, 1]),\n",
    "        'eta' : tune.loguniform(1e-4, 1e-1),  \n",
    "        'scale_pos_weight' : tune.choice([((len(y_train)-sum(y_train))/sum(y_train)),1]),\n",
    "        \n",
    "    }\n",
    "    \n",
    "\n",
    "    analysis = tune.run(\n",
    "        tree,\n",
    "        # You can add \"gpu\": 0.1 to allocate GPUs\n",
    "        resources_per_trial={\"cpu\": 8},\n",
    "        config=config,\n",
    "        verbose=1,\n",
    "        num_samples=200)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get best trial\n",
    "best_trial = analysis.get_best_trial(\"mean_auc\", \"max\")\n",
    "best_accuracy = best_trial.metric_analysis[\"mean_auc\"][\"last\"]\n",
    "\n",
    "\n",
    "best_config = analysis.get_best_config(\"mean_auc\", \"max\")\n",
    "print(best_accuracy)\n",
    "print(best_config)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importance, does not function well due to the many features caused by flattening the numpy array \n",
    "feature_important = boost.get_score(importance_type='weight')\n",
    "feature_important "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
